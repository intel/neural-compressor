:orphan:

:py:mod:`neural_compressor.adaptor.pytorch`
===========================================

.. py:module:: neural_compressor.adaptor.pytorch


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.adaptor.pytorch.TemplateAdaptor
   neural_compressor.adaptor.pytorch.PyTorchAdaptor
   neural_compressor.adaptor.pytorch.PyTorch_IPEXAdaptor
   neural_compressor.adaptor.pytorch.PyTorch_FXAdaptor



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.adaptor.pytorch.get_ops_recursively



.. py:function:: get_ops_recursively(model, prefix, ops={})

   This is a helper function for `graph_info`,
       and it will get all ops from model.
   :param model: input model
   :type model: object
   :param prefix: prefix of op name
   :type prefix: string
   :param ops: dict of ops from model {op name: type}.
   :type ops: dict

   :returns: None


.. py:class:: TemplateAdaptor(framework_specific_info)

   Bases: :py:obj:`neural_compressor.adaptor.adaptor.Adaptor`

   Tample adaptor of PyTorch framework.

   :param framework_specific_info: dictionary of tuning configure from yaml file.
   :type framework_specific_info: dict

   .. py:method:: is_fused_module(module)

      This is a helper function for `_propagate_qconfig_helper` to detecte
         if this module is fused.

      :param module: input module
      :type module: object

      :returns: is fused or not
      :rtype: (bool)


   .. py:method:: calculate_hessian_trace(fp32_model, dataloader, q_model, criterion, enable_act=False)

      Calculate hessian trace.

      :param fp32_model: The original fp32 model.
      :param criterion: The loss function for calculate the hessian trace. # loss = criterion(output, target)
      :param dataloader: The dataloader for calculate the gradient.
      :param q_model: The INT8 AMAP model.
      :param enable_act: Enabling quantization error or not.

      :returns: (op_name, op_type); value: hessian trace.
      :rtype: hessian_trace(Dict[Tuple, float]), key


   .. py:method:: smooth_quant(model, dataloader, calib_iter, tune_cfg=None, alpha=0.5, percentile=None, op_types=None, scales_per_op=None, force_re_smooth=False)

      convert the model by smooth quant.

      :param model: origin FP32 model
      :param dataloader: calib dataloader
      :param calib_iter: calib iters
      :param tune_cfg: quantization config
      :param alpha: smooth alpha in SmoothQuant, 1.0 will fallback to SPIQ
      :param percentile: Percentile of calibration to remove outliers, not supported now
      :param op_types: The op types whose input tensor will be dumped
      :param scales_per_op: True, each op will have an individual scale, mainly for accuracy
                            False, ops with the same input will share a scale, mainly for performance

      :returns: A modified fp32 model
      :rtype: model



.. py:class:: PyTorchAdaptor(framework_specific_info)

   Bases: :py:obj:`TemplateAdaptor`

   Adaptor of PyTorch framework, all PyTorch API is in this class.

   :param framework_specific_info: dictionary of tuning configure from yaml file.
   :type framework_specific_info: dict

   .. py:method:: quantize(tune_cfg, model, dataloader, q_func=None)

      Execute the quantize process on the specified model.

      :param tune_cfg: quantization config.
      :type tune_cfg: dict
      :param model: model need to do quantization.
      :type model: object
      :param dataloader: calibration dataset.
      :type dataloader: object
      :param q_func: training function for quantization aware training mode.
      :type q_func: objext, optional

      :returns: quantized model
      :rtype: (object)


   .. py:method:: evaluate(model, dataloader, postprocess=None, metrics=None, measurer=None, iteration=-1, tensorboard=False, fp32_baseline=False)

      Execute the evaluate process on the specified model.

      :param model: model to run evaluation.
      :type model: object
      :param dataloader: evaluation dataset.
      :type dataloader: object
      :param postprocess: process function after evaluation.
      :type postprocess: object, optional
      :param metrics: list of metric function.
      :type metrics: list, optional
      :param measurer: measurer function.
      :type measurer: object, optional
      :param iteration: number of iterations to evaluate.
      :type iteration: int, optional
      :param tensorboard: dump output tensor to tensorboard summary files.
      :type tensorboard: bool, optional
      :param fp32_baseline: only for compare_label=False pipeline
      :type fp32_baseline: boolen, optional

      :returns: accuracy
      :rtype: (object)


   .. py:method:: train(model, dataloader, optimizer_tuple, criterion_tuple, hooks, **kwargs)

      Execute the train process on the specified model.

      :param model: model to run evaluation.
      :type model: object
      :param dataloader: training dataset.
      :type dataloader: object
      :param optimizer: It is a tuple of (cls, parameters) for optimizer.
      :type optimizer: tuple
      :param criterion: It is a tuple of (cls, parameters) for criterion.
      :type criterion: tuple
      :param kwargs: other parameters.
      :type kwargs: dict, optional

      :returns: None


   .. py:method:: is_fused_child(op_name)

      This is a helper function for `_post_eval_hook`

      :param op_name: op name
      :type op_name: string

      :returns: if this op is fused
      :rtype: (bool)


   .. py:method:: is_fused_op(op_name)

      This is a helper function for `_post_eval_hook`

      :param op_name: op name
      :type op_name: string

      :returns: if this op is fused
      :rtype: (bool)


   .. py:method:: is_last_fused_child(op_name)

      This is a helper function for `_post_eval_hook`

      :param op_name: op name
      :type op_name: string

      :returns: if this op is last fused op
      :rtype: (bool)


   .. py:method:: query_fw_capability(model)

      This is a helper function to get all quantizable ops from model.

      :param model: input model which is Neural Compressor model
      :type model: object

      :returns: tuning capability for each op from model.
      :rtype: q_capability (dictionary)


   .. py:method:: get_non_quant_modules(model_kwargs)

      This is a helper function to get all non_quant_modules from customer and default.

      :param model_kwargs: keyword args from Neural Compressor model
      :type model_kwargs: dictionary

      :returns: non_quant_modules for model.
      :rtype: custom_non_quant_dict (dictionary)



.. py:class:: PyTorch_IPEXAdaptor(framework_specific_info)

   Bases: :py:obj:`TemplateAdaptor`

   Adaptor of PyTorch framework with Intel PyTorch Extension,
      all PyTorch IPEX API is in this class.

   :param framework_specific_info: dictionary of tuning configure from yaml file.
   :type framework_specific_info: dict

   .. py:method:: quantize(tune_cfg, model, dataloader, q_func=None)

      Execute the quantize process on the specified model.

      :param tune_cfg: quantization config.
      :type tune_cfg: dict
      :param model: model need to do quantization, it is Neural Compressor model.
      :type model: object
      :param dataloader: calibration dataset.
      :type dataloader: object
      :param q_func: training function for quantization aware training mode.
      :type q_func: objext, optional

      :returns: quantized model
      :rtype: (dict)


   .. py:method:: evaluate(model, dataloader, postprocess=None, metrics=None, measurer=None, iteration=-1, tensorboard=False, fp32_baseline=False)

      Execute the evaluate process on the specified model.

      :param model: Neural Compressor model to run evaluation.
      :type model: object
      :param dataloader: evaluation dataset.
      :type dataloader: object
      :param postprocess: process function after evaluation.
      :type postprocess: object, optional
      :param metrics: list of metric function.
      :type metrics: list, optional
      :param measurer: measurer function.
      :type measurer: object, optional
      :param iteration: number of iterations to evaluate.
      :type iteration: int, optional
      :param tensorboard: dump output tensor to tensorboard summary
                          files(IPEX unspport).
      :type tensorboard: bool, optional
      :param fp32_baseline: only for compare_label=False pipeline
      :type fp32_baseline: boolen, optional

      :returns: quantized model
      :rtype: (dict)


   .. py:method:: query_fw_capability(model)

      This is a helper function to get all quantizable ops from model.

      :param model: input model which is Neural Compressor model
      :type model: object

      :returns: tuning capability for each op from model.
      :rtype: q_capability (dictionary)


   .. py:method:: save(model, path=None)

      The function is used by tune strategy class for set best configure in Neural Compressor model.

         Args:
             model (object): The Neural Compressor model which is best results.
             path (string): No used.

      :returns: None



.. py:class:: PyTorch_FXAdaptor(framework_specific_info)

   Bases: :py:obj:`TemplateAdaptor`

   Adaptor of PyTorch framework with FX graph mode, all PyTorch API is in this class.

   :param framework_specific_info: dictionary of tuning configure from yaml file.
   :type framework_specific_info: dict

   .. py:method:: quantize(tune_cfg, model, dataloader, q_func=None)

      Execute the quantize process on the specified model.

      :param tune_cfg: quantization config.
      :type tune_cfg: dict
      :param model: model need to do quantization.
      :type model: object
      :param dataloader: calibration dataset.
      :type dataloader: object
      :param q_func: training function for quantization aware training mode.
      :type q_func: objext, optional

      :returns: quantized model
      :rtype: (object)


   .. py:method:: evaluate(model, dataloader, postprocess=None, metrics=None, measurer=None, iteration=-1, tensorboard=False, fp32_baseline=False)

      Execute the evaluate process on the specified model.

      :param model: model to run evaluation.
      :type model: object
      :param dataloader: evaluation dataset.
      :type dataloader: object
      :param postprocess: process function after evaluation.
      :type postprocess: object, optional
      :param metric: metric function.
      :type metric: object, optional
      :param measurer: measurer function.
      :type measurer: object, optional
      :param iteration: number of iterations to evaluate.
      :type iteration: int, optional
      :param tensorboard: dump output tensor to tensorboard summary files.
      :type tensorboard: bool, optional
      :param fp32_baseline: only for compare_label=False pipeline
      :type fp32_baseline: boolen, optional

      :returns: accuracy
      :rtype: (object)


   .. py:method:: train(model, dataloader, optimizer_tuple, criterion_tuple, hooks, **kwargs)

      Execute the train process on the specified model.

      :param model: model to run evaluation.
      :type model: object
      :param dataloader: training dataset.
      :type dataloader: object
      :param optimizer: It is a tuple of (cls, parameters) for optimizer.
      :type optimizer: tuple
      :param criterion: It is a tuple of (cls, parameters) for criterion.
      :type criterion: tuple
      :param kwargs: other parameters.
      :type kwargs: dict, optional

      :returns: None


   .. py:method:: prepare_sub_graph(sub_module_list, fx_op_cfgs, model, prefix, is_qat=False, example_inputs=None, custom_config=None)
      :staticmethod:

      Static method to prepare sub modules recursively.

      :param sub_module_list: contains the name of traceable sub modules
      :type sub_module_list: list
      :param fx_op_cfgs: the configuration for prepare_fx quantization.
      :type fx_op_cfgs: dict, QConfigMapping
      :param model: input model which is PyTorch model.
      :type model: dir
      :param prefix: prefix of op name
      :type prefix: string
      :param is_qat: whether it is a qat quantization
      :type is_qat: bool
      :param example_inputs: example inputs
      :type example_inputs: tensor / tupe of tensor
      :param custom_config: custom non traceable module dict
      :type custom_config: dict

      :returns: output model which is a prepared PyTorch model.
      :rtype: model (dir)


   .. py:method:: convert_sub_graph(sub_module_list, model, prefix, custom_config=None)
      :staticmethod:

      Static method to convert sub modules recursively.

      :param sub_module_list: contains the name of traceable sub modules
      :type sub_module_list: list
      :param model: input model which is prepared PyTorch model.
      :type model: dir
      :param prefix: prefix of op name
      :type prefix: string
      :param custom_config: custom non traceable module dict
      :type custom_config: dict

      :returns: output model which is a converted PyTorch int8 model.
      :rtype: model (dir)


   .. py:method:: query_fw_capability(model)

      This is a helper function to get all quantizable ops from model.

      :param model: input model which is Neural Compressor model
      :type model: object

      :returns: tuning capability for each op from model.
      :rtype: q_capability (dictionary)


   .. py:method:: fuse_fx_model(model, is_qat)

      This is a helper function to get fused fx model for PyTorch_FXAdaptor.

      :param model: input model which is Neural Compressor model.
      :type model: object
      :param is_qat: check quantization approach is qat or not.
      :type is_qat: bool

      :returns: fused GraphModule model from torch.fx.
      :rtype: fused_model (GraphModule)


   .. py:method:: calculate_op_sensitivity(model, dataloader, tune_cfg, output_op_names, confidence_batches, fallback=True, requantize_cfgs=None)

      This is a helper function for `query_fw_capability`,
         and it will get all quantizable ops from model.

      :param model: INC model containing fp32 model
      :type model: object
      :param dataloader: dataloader contains real data.
      :type dataloader: string
      :param tune_cfg: dictionary of tune configure for each op.
      :type tune_cfg: dict
      :param fallback: switch method in fallback stage and re-quantize stage
      :type fallback: bool

      :returns: sorted op list by sensitivity
      :rtype: ops_lst (list)



