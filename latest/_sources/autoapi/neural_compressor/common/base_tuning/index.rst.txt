:orphan:

:py:mod:`neural_compressor.common.base_tuning`
==============================================

.. py:module:: neural_compressor.common.base_tuning


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.Evaluator
   neural_compressor.common.base_tuning.ConfigSet
   neural_compressor.common.base_tuning.Sampler
   neural_compressor.common.base_tuning.SequentialSampler
   neural_compressor.common.base_tuning.ConfigLoader
   neural_compressor.common.base_tuning.TuningLogger
   neural_compressor.common.base_tuning.TuningConfig
   neural_compressor.common.base_tuning.TuningMonitor



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.init_tuning



Attributes
~~~~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.default_sampler


.. py:class:: Evaluator


   Evaluator is a collection of evaluation functions.

   .. rubric:: Examples

   def eval_acc(model):
       ...

   def eval_perf(molde):
       ...

   # Usage
   user_eval_fns1 = eval_acc
   user_eval_fns2 = {"eval_fn": eval_acc}
   user_eval_fns3 = {"eval_fn": eval_acc, "weight": 1.0, "name": "accuracy"}
   user_eval_fns4 = [
       {"eval_fn": eval_acc, "weight": 0.5},
       {"eval_fn": eval_perf, "weight": 0.5, "name": "accuracy"},
       ]




.. py:class:: SequentialSampler(config_source: Sized)




   Samples elements sequentially, always in the same order.

   :param config_source: config set to sample from
   :type config_source: _ConfigSet




.. py:class:: TuningLogger


   A unified logger for the tuning process.

   It assists validation teams in retrieving logs.


.. py:class:: TuningConfig(config_set=None, max_trials=100, sampler: Sampler = default_sampler, tolerable_loss=0.01)


   Base Class for Tuning Criterion.

   :param config_set: quantization configs. Default value is empty.
                      A single config or a list of configs. More details can
                      be found in the `from_fwk_configs`of `ConfigSet` class.
   :param max_trials: Max tuning times. Default value is 100. Combine with timeout field to decide when to exit.
   :param tolerable_loss: This float indicates how much metric loss we can accept.             The metric loss is relative, it can be both positive and negative. Default is 0.01.

   .. rubric:: Examples

   # TODO: to refine it
   from neural_compressor import TuningConfig
   tune_config = TuningConfig(
       config_set=[config1, config2, ...],
       max_trials=3,
       tolerable_loss=0.01
   )

   # Case 1: Tolerable Loss
   fp32_baseline = 100
   config1_metric, config2_metric, ... = 98, 99, ...

   # Tuning result of case 1:
   # The best tuning config is config2, because config2_metric >= fp32_baseline * (1 - tolerable_loss)

   # Case 2: Maximum Trials
   fp32_baseline = 100
   config1_metric, config2_metric, config3_metric, ... = 98, 98, 97, ...

   # Tuning result of case 2:
   # The best tuning config is config2, because of the following:
   # 1. Not achieving the set goal. (config_metric < fp32_baseline * (1 - tolerable_loss))
   # 2. Reached maximum tuning times.




