

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FX &mdash; Intel® Neural Compressor 3.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">FX</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/source/FX.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="fx">
<h1>FX<a class="headerlink" href="#fx" title="Link to this heading"></a></h1>
<ol>
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#fx-mode-support-matrix-in-neural-compressor">FX Mode Support Matrix in Neural Compressor</a></p></li>
<li><p><a class="reference external" href="#get-started">Get Started</a></p>
<p>3.1. <a class="reference external" href="#post-training-static-quantization">Post Training Static Quantization</a></p>
<p>3.2. <a class="reference external" href="#post-training-dynamic-quantization">Post Training Dynamic Quantization</a></p>
<p>3.3. <a class="reference external" href="#quantization-aware-training">Quantization-Aware Training</a></p>
</li>
<li><p><a class="reference external" href="#examples">Examples</a></p></li>
<li><p><a class="reference external" href="#note">Note</a></p></li>
<li><p><a class="reference external" href="#common-problem">Common Problem</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>FX is a PyTorch toolkit for developers to use to transform nn.Module instance. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation.</p>
<p>With converted torch.fx.GraphModule, we can resolve three problems in quantization:</p>
<ol class="simple">
<li><p>Automatically insert quant/dequant operation within PyTorch.</p></li>
<li><p>Use FloatFunctional to wrap tensor operations that require special handling for quantization into modules. Examples are operations like add and cat which require special handling to determine output quantization parameters.</p></li>
<li><p>Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the fuse_modules() API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu].</p></li>
</ol>
<p>For detailed description, please refer to <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">PyTorch FX</a> and <a class="reference external" href="https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization">FX Graph Mode Quantization</a></p>
</section>
<section id="fx-mode-support-matrix-in-neural-compressor">
<h2>FX Mode Support Matrix in Neural Compressor<a class="headerlink" href="#fx-mode-support-matrix-in-neural-compressor" title="Link to this heading"></a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th>quantization</th>
<th style="text-align: center;">FX</th>
</tr>
</thead>
<tbody>
<tr>
<td>Static Quantization</td>
<td style="text-align: center;">&#10004;</td>
</tr>
<tr>
<td>Dynamic Quantization</td>
<td style="text-align: center;">&#10004;</td>
</tr>
<tr>
<td>Quantization-Aware Training</td>
<td style="text-align: center;">&#10004;</td>
</tr>
</tbody>
</table></section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading"></a></h2>
<p><strong>Note:</strong> “backend” field indicates the backend used by the user in configure. And the “default” value means it will quantization model with fx backend for PyTorch model.</p>
<section id="post-training-static-quantization">
<h3>Post Training Static Quantization<a class="headerlink" href="#post-training-static-quantization" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantization</span><span class="p">,</span> <span class="n">PostTrainingQuantConfig</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">PostTrainingQuantConfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">q_model</span> <span class="o">=</span> <span class="n">quantization</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">calib_dataloader</span><span class="o">=</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">)</span>
    <span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;save/to/path&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="post-training-dynamic-quantization">
<h3>Post Training Dynamic Quantization<a class="headerlink" href="#post-training-dynamic-quantization" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantization</span><span class="p">,</span> <span class="n">PostTrainingQuantConfig</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">PostTrainingQuantConfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">approach</span><span class="o">=</span><span class="s2">&quot;dynamic&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">q_model</span> <span class="o">=</span> <span class="n">quantization</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">)</span>
    <span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;save/to/path&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quantization-aware-training">
<h3>Quantization-Aware Training<a class="headerlink" href="#quantization-aware-training" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationAwareTrainingConfig</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.training</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare_compression</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">QuantizationAwareTrainingConfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">)</span>
    <span class="n">compression_manager</span> <span class="o">=</span> <span class="n">prepare_compression</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">)</span>
    <span class="n">compression_manager</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_begin</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">compression_manager</span><span class="o">.</span><span class="n">model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="c1">####### Training loop #####</span>

    <span class="n">compression_manager</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;save/to/path&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<p>User could refer to <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/question-answering/quantization/ptq_static/fx/README.html">examples</a> on how to quantize a model with FX backend.</p>
</section>
<section id="note">
<h2>Note<a class="headerlink" href="#note" title="Link to this heading"></a></h2>
<p>Right now, we support auto quantization method and can avoid below common problem.<br />For users, you will see log output below if you model failed on symbolic trace method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">INFO</span><span class="p">]</span> <span class="n">Fx</span> <span class="n">trace</span> <span class="n">of</span> <span class="n">the</span> <span class="n">entire</span> <span class="n">model</span> <span class="n">failed</span><span class="o">.</span> <span class="n">We</span> <span class="n">will</span> <span class="n">conduct</span> <span class="n">auto</span> <span class="n">quantization</span>
</pre></div>
</div>
<section id="details">
<h3>Details<a class="headerlink" href="#details" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>We combine GraphModule from symbolic_trace and imperative control flow. Therefore, the INT8 model consists of lots of GraphModules.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="common-problem">
<h2>Common Problem<a class="headerlink" href="#common-problem" title="Link to this heading"></a></h2>
<section id="dynamic-quantization">
<h3><em>Dynamic Quantization</em><a class="headerlink" href="#dynamic-quantization" title="Link to this heading"></a></h3>
<ul>
<li><p>PyTorch Version: 1.9 or higher</p>
<p>You can use pytorch backend for dynamic quantization, there is no difference between pytorch and pytorch_fx. we don’t need to trace model because we don’t need to modify the source code of the model.</p>
</li>
</ul>
</section>
<section id="static-quantization-quantization-aware-training">
<h3><em>Static Quantization</em> &amp; <em>Quantization Aware Training</em><a class="headerlink" href="#static-quantization-quantization-aware-training" title="Link to this heading"></a></h3>
<ul>
<li><p>PyTorch Version: 1.8 or higher</p>
<p>As symbolic trace cannot handle dynamic control, tensor iteration and so on, we might meet trace failure sometimes. In order to quantize the model successfully, we suggest user to trace model with below two approaches first and then pass it to neural_compressor.</p>
<ol>
<li><p>Non_traceable_module_class/name</p>
<p>Select module classes or names that cannot be traced by proxy object, and pass them into prepare_fx as a dict.</p>
<p><strong>Please refer to:</strong> https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=non_traceable_module_class</p>
</li>
<li><p>Decorator: &#64;torch.fx.wrap</p>
<p>If untraceable part is not a part of a module, like a global function called, or you want to move untraceable part out of model to keep other parts get quantized, you should try to use Decorator <code class="docutils literal notranslate"><span class="pre">&#64;torch.fx.wrap</span></code>. The wrapped function must be in the global, not in the class.</p>
<p><strong>For example:</strong> examples/pytorch/fx/object_detection/ssd_resnet34/ptq/python/models/ssd_r34.py</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">wrap</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bboxes_labels_scores</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">criteria</span> <span class="o">=</span> <span class="mf">0.45</span><span class="p">,</span> <span class="n">max_output</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">boxes</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">labels</span><span class="o">=</span><span class="p">[];</span> <span class="n">scores</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bboxes</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">probs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
        <span class="n">bbox</span> <span class="o">=</span> <span class="n">bbox</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">dbox</span><span class="p">,</span><span class="n">dlabel</span><span class="p">,</span><span class="n">dscore</span><span class="o">=</span><span class="n">decode_single</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">criteria</span><span class="p">,</span> <span class="n">max_output</span><span class="p">)</span>
        <span class="n">boxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dbox</span><span class="p">)</span>
        <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dlabel</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dscore</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">boxes</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">scores</span><span class="p">]</span>
    <span class="err">```</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fe209933b30> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>