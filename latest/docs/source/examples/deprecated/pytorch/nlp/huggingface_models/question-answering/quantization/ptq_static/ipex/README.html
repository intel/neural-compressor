

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Step-by-Step &mdash; Intel® Neural Compressor 3.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../../../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../../../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Step-by-Step</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../../../../_sources/docs/source/examples/deprecated/pytorch/nlp/huggingface_models/question-answering/quantization/ptq_static/ipex/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="step-by-step">
<h1>Step-by-Step<a class="headerlink" href="#step-by-step" title="Link to this heading"></a></h1>
<p>This document describes the step-by-step instructions for reproducing Huggingface models with IPEX backend tuning results with Intel® Neural Compressor.</p>
<blockquote>
<div><p>Note: IPEX version &gt;= 1.10</p>
</div></blockquote>
</section>
<section id="prerequisite">
<h1>Prerequisite<a class="headerlink" href="#prerequisite" title="Link to this heading"></a></h1>
<section id="environment">
<h2>1. Environment<a class="headerlink" href="#environment" title="Link to this heading"></a></h2>
<p>Recommend python 3.6 or higher version.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>examples/pytorch/nlp/huggingface_models/question-answering/quantization/ptq_static/ipex
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip<span class="w"> </span>install<span class="w"> </span>torch
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>intel_extension_for_pytorch<span class="w"> </span>-f<span class="w"> </span>https://software.intel.com/ipex-whl-stable
</pre></div>
</div>
<blockquote>
<div><p>Note: Intel® Extension for PyTorch* has PyTorch version requirement. Please check more detailed information via the URL below.</p>
</div></blockquote>
</section>
</section>
<section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading"></a></h1>
<section id="quantization-with-cpu">
<h2>1. Quantization with CPU<a class="headerlink" href="#quantization-with-cpu" title="Link to this heading"></a></h2>
<p>If IPEX version is equal or higher than 1.12, please install transformers 4.19.0.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>run_qa.py<span class="w"> </span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>bert-large-uncased-whole-word-masking-finetuned-squad<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_name<span class="w"> </span>squad<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--do_eval<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">384</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--doc_stride<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tune<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>./savedresult
</pre></div>
</div>
<blockquote>
<div><p>NOTE</p>
<p>/path/to/checkpoint/dir is the path to finetune output_dir</p>
</div></blockquote>
</section>
<section id="quantization-with-xpu">
<h2>2. Quantization with XPU<a class="headerlink" href="#quantization-with-xpu" title="Link to this heading"></a></h2>
<p>Please build an IPEX docker container with following steps. Please also refer to the <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/xpu-master/docker">official guide</a>.</p>
<section id="build-container-and-environment-variables">
<h3>2.1 Build Container and Environment Variables<a class="headerlink" href="#build-container-and-environment-variables" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://raw.githubusercontent.com/intel/intel-extension-for-pytorch/xpu-master/docker/Dockerfile.xpu
wget<span class="w"> </span>https://raw.githubusercontent.com/intel/intel-extension-for-pytorch/xpu-master/docker/build.sh
./build.sh<span class="w"> </span>xpu-flex

<span class="nb">export</span><span class="w"> </span><span class="nv">IMAGE_NAME</span><span class="o">=</span>intel/intel-extension-for-pytorch:xpu-flex
<span class="nb">export</span><span class="w"> </span><span class="nv">VIDEO</span><span class="o">=</span><span class="k">$(</span>getent<span class="w"> </span>group<span class="w"> </span>video<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-E<span class="w"> </span><span class="s1">&#39;s,^video:[^:]*:([^:]*):.*$,\1,&#39;</span><span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RENDER</span><span class="o">=</span><span class="k">$(</span>getent<span class="w"> </span>group<span class="w"> </span>render<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-E<span class="w"> </span><span class="s1">&#39;s,^render:[^:]*:([^:]*):.*$,\1,&#39;</span><span class="k">)</span>
<span class="nb">test</span><span class="w"> </span>-z<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$RENDER</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nv">RENDER_GROUP</span><span class="o">=</span><span class="s2">&quot;--group-add </span><span class="si">${</span><span class="nv">RENDER</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</section>
<section id="run-container">
<h3>2.2 Run Container<a class="headerlink" href="#run-container" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>.:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span><span class="si">${</span><span class="nv">VIDEO</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="si">${</span><span class="nv">RENDER_GROUP</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-e<span class="w"> </span><span class="nv">http_proxy</span><span class="o">=</span><span class="nv">$http_proxy</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-e<span class="w"> </span><span class="nv">https_proxy</span><span class="o">=</span><span class="nv">$https_proxy</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-e<span class="w"> </span><span class="nv">no_proxy</span><span class="o">=</span><span class="nv">$no_proxy</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-it<span class="w"> </span><span class="nv">$IMAGE_NAME</span><span class="w"> </span>bash
</pre></div>
</div>
</section>
<section id="environment-settings">
<h3>2.3 Environment Settings<a class="headerlink" href="#environment-settings" title="Link to this heading"></a></h3>
<p>Please set basekit configurations as following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>l_BaseKit_p_2024.0.0.49261_offline.sh<span class="w"> </span>-a<span class="w"> </span>-s<span class="w"> </span>--eula<span class="w"> </span>accept<span class="w"> </span>--components<span class="w"> </span>intel.oneapi.lin.tbb.devel:intel.oneapi.lin.ccl.devel:intel.oneapi.lin.mkl.devel:intel.oneapi.lin.dpcpp-cpp-compiler<span class="w"> </span>--install-dir<span class="w"> </span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi
<span class="nb">source</span><span class="w"> </span>./20240921_xmainrel/env/vars.sh
<span class="c1"># source ${HOME}/intel/oneapi/compiler/latest/env/vars.sh</span>
<span class="nb">source</span><span class="w"> </span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi/mkl/latest/env/vars.sh
<span class="nb">source</span><span class="w"> </span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi/tbb/latest/env/vars.sh
<span class="nb">export</span><span class="w"> </span><span class="nv">MKL_DPCPP_ROOT</span><span class="o">=</span><span class="si">${</span><span class="nv">MKLROOT</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib64:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib64:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64:<span class="nv">$LIBRARY_PATH</span>
</pre></div>
</div>
<p>Prebuilt wheel files are available for Python python 3.8, python 3.9, python 3.10, python 3.11.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>intel-extension-for-pytorch<span class="o">=</span><span class="m">2</span>.0.110<span class="w"> </span><span class="nv">pytorch</span><span class="o">=</span><span class="m">2</span>.0.1<span class="w"> </span>-c<span class="w"> </span>intel<span class="w"> </span>-c<span class="w"> </span>conda-forge
</pre></div>
</div>
<p>You can run a simple sanity test to double confirm if the correct version is installed, and if the software stack can get correct hardware information onboard your system. The command should return PyTorch and IPEX versions installed, as well as GPU card(s) information detected.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span><span class="o">{</span>DPCPPROOT<span class="o">}</span>/env/vars.sh
<span class="nb">source</span><span class="w"> </span><span class="o">{</span>MKLROOT<span class="o">}</span>/env/vars.sh
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f&#39;[{i}]: {torch.xpu.get_device_properties(i)}&#39;) for i in range(torch.xpu.device_count())];&quot;</span>
</pre></div>
</div>
<p>Please also refer to this <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.0.110%2Bxpu">tutorial</a> to check system requirements and install dependencies.</p>
</section>
<section id="quantization-command">
<h3>2.4 Quantization Command<a class="headerlink" href="#quantization-command" title="Link to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>run_qa.py<span class="w"> </span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>bert-large-uncased-whole-word-masking-finetuned-squad<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_name<span class="w"> </span>squad<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--do_eval<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">384</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--doc_stride<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--xpu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tune<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>./savedresult
</pre></div>
</div>
</section>
</section>
</section>
<section id="tutorial-of-how-to-enable-nlp-model-with-intel-neural-compressor">
<h1>Tutorial of How to Enable NLP Model with Intel® Neural Compressor.<a class="headerlink" href="#tutorial-of-how-to-enable-nlp-model-with-intel-neural-compressor" title="Link to this heading"></a></h1>
<section id="intel-neural-compressor-supports-two-usages">
<h2>Intel® Neural Compressor supports two usages:<a class="headerlink" href="#intel-neural-compressor-supports-two-usages" title="Link to this heading"></a></h2>
<ol class="simple">
<li><p>User specifies fp32 ‘model’, calibration dataset ‘q_dataloader’, evaluation dataset “eval_dataloader” and metrics.</p></li>
<li><p>User specifies fp32 ‘model’, calibration dataset ‘q_dataloader’ and a custom “eval_func” which encapsulates the evaluation dataset and metrics by itself.</p></li>
</ol>
<p>As MRPC’s metrics are ‘f1’, ‘acc_and_f1’, mcc’, ‘spearmanr’, ‘acc’, so customer should provide evaluation function ‘eval_func’, it’s suitable for the second use case.</p>
</section>
<section id="code-prepare">
<h2>Code Prepare<a class="headerlink" href="#code-prepare" title="Link to this heading"></a></h2>
<p>We just need update run_qa.py like below</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="c1"># Initialize our Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span> <span class="k">if</span> <span class="n">training_args</span><span class="o">.</span><span class="n">do_train</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span> <span class="k">if</span> <span class="n">training_args</span><span class="o">.</span><span class="n">do_eval</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eval_examples</span><span class="o">=</span><span class="n">eval_examples</span> <span class="k">if</span> <span class="n">training_args</span><span class="o">.</span><span class="n">do_eval</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">post_process_function</span><span class="o">=</span><span class="n">post_processing_function</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">get_eval_dataloader</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">eval_dataloader</span><span class="o">.</span><span class="n">batch_size</span>
<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">&quot;eval_f1&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">take_eval_steps</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">save_metrics</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">metric_name</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">eval_func</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">take_eval_steps</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">)</span>

<span class="n">ipex</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_model_convert</span><span class="o">.</span><span class="n">replace_dropout_with_identity</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">PostTrainingQuantConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantization</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">PostTrainingQuantConfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;ipex&quot;</span><span class="p">,</span> <span class="n">calibration_sampling_size</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantization</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                           <span class="n">conf</span><span class="p">,</span>
                           <span class="n">calib_dataloader</span><span class="o">=</span><span class="n">eval_dataloader</span><span class="p">,</span>
                           <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">)</span>
<span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">training_args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fe9bcd5dd90> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>