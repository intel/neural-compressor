

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Step-by-Step &mdash; Intel® Neural Compressor 3.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Step-by-Step</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../../_sources/docs/source/examples/pytorch/image_recognition/torchvision_models/quantization/static_quant/ipex/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="step-by-step">
<h1>Step-by-Step<a class="headerlink" href="#step-by-step" title="Link to this heading"></a></h1>
<p>This document describes the step-by-step instructions for reproducing PyTorch tuning results with Intel® Neural Compressor.</p>
</section>
<section id="prerequisite">
<h1>Prerequisite<a class="headerlink" href="#prerequisite" title="Link to this heading"></a></h1>
<section id="environment">
<h2>1. Environment<a class="headerlink" href="#environment" title="Link to this heading"></a></h2>
<p>We verified examples with IPEX backend on Python 3.10, recommended.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</section>
<section id="install-intel-pytorch-extension">
<h2>2. Install Intel-Pytorch-Extension<a class="headerlink" href="#install-intel-pytorch-extension" title="Link to this heading"></a></h2>
<p>Please refer to <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">intel/intel-extension-for-pytorch(github.com)</a>.</p>
<section id="install-ipex-cpu">
<h3>Install IPEX CPU<a class="headerlink" href="#install-ipex-cpu" title="Link to this heading"></a></h3>
<blockquote>
<div><p>Note: GCC9 compiler is recommended</p>
</div></blockquote>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>intel_extension_for_pytorch<span class="w"> </span>-f<span class="w"> </span>https://software.intel.com/ipex-whl-stable
</pre></div>
</div>
</section>
<section id="install-ipex-intel-gpu">
<h3>Install IPEX Intel GPU<a class="headerlink" href="#install-ipex-intel-gpu" title="Link to this heading"></a></h3>
<p>Please build an IPEX docker container according to the <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.1.30%2bxpu&amp;os=linux%2fwsl2&amp;package=docker">official guide</a>.</p>
<p>You can run a simple sanity test to double confirm if the correct version is installed, and if the software stack can get correct hardware information onboard your system. The command should return PyTorch and IPEX versions installed, as well as GPU card(s) information detected.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span><span class="o">{</span>DPCPPROOT<span class="o">}</span>/env/vars.sh
<span class="nb">source</span><span class="w"> </span><span class="o">{</span>MKLROOT<span class="o">}</span>/env/vars.sh
<span class="nb">source</span><span class="w"> </span><span class="o">{</span>CCLROOT<span class="o">}</span>/env/vars.sh
<span class="nb">source</span><span class="w"> </span><span class="o">{</span>MPIROOT<span class="o">}</span>/env/vars.sh
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f&#39;[{i}]: {torch.xpu.get_device_properties(i)}&#39;) for i in range(torch.xpu.device_count())];&quot;</span>
</pre></div>
</div>
<p>Please also refer to this <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.1.30%2bxpu&amp;os=linux%2fwsl2&amp;package=conda">tutorial</a> to check system requirements and install dependencies.</p>
</section>
</section>
<section id="prepare-dataset">
<h2>3. Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Link to this heading"></a></h2>
<p>Download <a class="reference external" href="http://www.image-net.org/">ImageNet</a> Raw image to dir: /path/to/imagenet. The dir include below folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ls<span class="w"> </span>/path/to/imagenet
train<span class="w">  </span>val
</pre></div>
</div>
</section>
</section>
<section id="run-with-cpu">
<h1>Run with CPU<a class="headerlink" href="#run-with-cpu" title="Link to this heading"></a></h1>
<blockquote>
<div><p>Note: All torchvision model names can be passed as long as they are included in <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>, below are some examples.</p>
</div></blockquote>
<section id="resnet18-with-intel-pytorch-extension">
<h2>1. ResNet18 With Intel PyTorch Extension<a class="headerlink" href="#resnet18-with-intel-pytorch-extension" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>-t<span class="w"> </span>-a<span class="w"> </span>resnet18<span class="w"> </span>--ipex<span class="w"> </span>--pretrained<span class="w"> </span>/path/to/imagenet
</pre></div>
</div>
<p>or</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_quant.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet18<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet
bash<span class="w"> </span>run_benchmark.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet18<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet<span class="w"> </span>--mode<span class="o">=</span>performance/accuracy<span class="w"> </span>--optimized<span class="o">=</span>true/false
</pre></div>
</div>
</section>
<section id="resnet50-with-intel-pytorch-extension">
<h2>2. ResNet50 With Intel PyTorch Extension<a class="headerlink" href="#resnet50-with-intel-pytorch-extension" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>-t<span class="w"> </span>-a<span class="w"> </span>resnet50<span class="w"> </span>--ipex<span class="w"> </span>--pretrained<span class="w"> </span>/path/to/imagenet
</pre></div>
</div>
<p>or</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_quant.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet50<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet
bash<span class="w"> </span>run_benchmark.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet50<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet<span class="w"> </span>--mode<span class="o">=</span>performance/accuracy<span class="w"> </span>--optimized<span class="o">=</span>true/false
</pre></div>
</div>
</section>
<section id="resnext101-32x16d-with-intel-pytorch-extension">
<h2>3. ResNext101_32x16d With Intel PyTorch Extension<a class="headerlink" href="#resnext101-32x16d-with-intel-pytorch-extension" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>-t<span class="w"> </span>-a<span class="w"> </span>resnext101_32x16d_wsl<span class="w"> </span>--hub<span class="w"> </span>--ipex<span class="w"> </span>--pretrained<span class="w"> </span>/path/to/imagenet
</pre></div>
</div>
<p>or</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_quant.sh<span class="w"> </span>--input_model<span class="o">=</span>resnext101_32x16d_wsl<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet
bash<span class="w"> </span>run_benchmark.sh<span class="w"> </span>--input_model<span class="o">=</span>resnext101_32x16d_wsl<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet<span class="w"> </span>--mode<span class="o">=</span>performance/accuracy<span class="w"> </span>--optimized<span class="o">=</span>true/false
</pre></div>
</div>
</section>
</section>
<section id="run-with-intel-gpu">
<h1>Run with Intel GPU<a class="headerlink" href="#run-with-intel-gpu" title="Link to this heading"></a></h1>
<blockquote>
<div><p>Note: All torchvision model names can be passed as long as they are included in <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>, below are some examples.</p>
</div></blockquote>
<section id="id1">
<h2>1. ResNet18 With Intel PyTorch Extension<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>-t<span class="w"> </span>-a<span class="w"> </span>resnet18<span class="w"> </span>--ipex<span class="w"> </span>--pretrained<span class="w"> </span>/path/to/imagenet<span class="w"> </span>--xpu
</pre></div>
</div>
<p>or</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_quant.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet18<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet
bash<span class="w"> </span>run_benchmark.sh<span class="w"> </span>--input_model<span class="o">=</span>resnet18<span class="w"> </span>--dataset_location<span class="o">=</span>/path/to/imagenet<span class="w"> </span>--mode<span class="o">=</span>performance/accuracy<span class="w"> </span>--optimized<span class="o">=</span>true/false<span class="w"> </span>--xpu<span class="o">=</span>true/false
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f879a811670> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>