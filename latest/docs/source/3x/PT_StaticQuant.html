

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PyTorch Static Quantization &mdash; Intel® Neural Compressor 3.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">PyTorch Static Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/source/3x/PT_StaticQuant.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pytorch-static-quantization">
<h1>PyTorch Static Quantization<a class="headerlink" href="#pytorch-static-quantization" title="Link to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#get-started">Get Started</a> <br />2.1 <a class="reference external" href="#static-quantization-with-ipex-backend">Static Quantization with IPEX Backend</a> <br />2.1.1 <a class="reference external" href="#usage-sample-with-ipex">Usage Sample with IPEX</a> <br />2.1.2 <a class="reference external" href="#specify-quantization-rules">Specify Quantization Rules</a> <br />2.1.3 <a class="reference external" href="#model-examples">Model Examples</a> <br />2.2 <a class="reference external" href="#static-quantization-with-pt2e-backend">Static Quantization with PT2E Backend</a> <br />2.2.1 <a class="reference external" href="#usage-sample-with-pt2e">Usage Sample with PT2E</a>
2.2.2 <a class="reference external" href="#model-examples-with-pt2e">Model Examples with PT2E</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Post-Training Quantization (PTQ) is a technique used to convert a pre-trained floating-point model to a quantized model. This approach does not require model retraining. Instead, it uses calibration data to determine the optimal quantization parameters. Static quantization involves calibrating both weights and activations during the quantization process. Currently, we support two paths to perform static PTQ: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Intel Extension for PyTorch (IPEX)</a> and <a class="reference external" href="https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization (PT2E)</a>.</p>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading"></a></h2>
<section id="static-quantization-with-ipex-backend">
<h3>Static Quantization with IPEX Backend<a class="headerlink" href="#static-quantization-with-ipex-backend" title="Link to this heading"></a></h3>
<p>Intel Extension for PyTorch (IPEX) provides optimizations specifically for Intel hardware, improving the performance of PyTorch models through efficient execution on CPUs. IPEX supports PTQ, allowing users to quantize models to lower precision to reduce model size and inference time while maintaining accuracy.</p>
<p>The design philosophy of the quantization interface of Intel(R) Neural Compressor is easy-of-use. It requests user to provide <code class="docutils literal notranslate"><span class="pre">model</span></code>, <code class="docutils literal notranslate"><span class="pre">calibration</span> <span class="pre">function</span></code>, and <code class="docutils literal notranslate"><span class="pre">example</span> <span class="pre">inputs</span></code>. Those parameters would be used to quantize and tune the model.</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span></code> is the framework model location or the framework model object.</p>
<p><code class="docutils literal notranslate"><span class="pre">calibration</span> <span class="pre">function</span></code> is used to determine the appropriate quantization parameters, such as <code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">zero-point</span></code>, for the model’s weights and activations. This process is crucial for minimizing the loss of accuracy that can occur when converting from floating-point to lower-precision format.</p>
<p>IPEX leverages just-in-time (JIT) compilation techniques for optimizing the model. <code class="docutils literal notranslate"><span class="pre">example</span> <span class="pre">inputs</span></code> is used to trace the computational graph of the model, enabling various optimizations and transformations that are specific to IPEX. This tracing process captures the operations performed by the model, allowing IPEX to apply quantization optimizations effectively. <code class="docutils literal notranslate"><span class="pre">example</span> <span class="pre">inputs</span></code> should be representative of the actual data the model will process to ensure accurate calibration.</p>
<section id="usage-sample-with-ipex">
<h4>Usage Sample with IPEX<a class="headerlink" href="#usage-sample-with-ipex" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.torch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">StaticQuantConfig</span><span class="p">,</span> <span class="n">convert</span><span class="p">,</span> <span class="n">prepare</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="n">StaticQuantConfig</span><span class="p">(</span><span class="n">act_sym</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">act_algo</span><span class="o">=</span><span class="s2">&quot;minmax&quot;</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="n">run_fn</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>[!IMPORTANT]<br />To use static quantization with the IPEX backend, please explicitly import IPEX at the beginning of your program.</p>
</div></blockquote>
</section>
<section id="specify-quantization-rules">
<h4>Specify Quantization Rules<a class="headerlink" href="#specify-quantization-rules" title="Link to this heading"></a></h4>
<p>Intel(R) Neural Compressor support specify quantization rules by operator name or operator type. Users can use <code class="docutils literal notranslate"><span class="pre">set_local</span></code> to fallback either <code class="docutils literal notranslate"><span class="pre">op_name</span></code> or <code class="docutils literal notranslate"><span class="pre">op_type</span></code> in <code class="docutils literal notranslate"><span class="pre">StaticQuantConfig</span></code> to achieve the above purpose.</p>
<ol class="simple">
<li><p>Example of <code class="docutils literal notranslate"><span class="pre">op_name_dict</span></code>
Here we don’t quantize the layer named <code class="docutils literal notranslate"><span class="pre">fc1</span></code>.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># fallback by op_name</span>
<span class="n">quant_config</span><span class="o">.</span><span class="n">set_local</span><span class="p">(</span><span class="s2">&quot;fc1&quot;</span><span class="p">,</span> <span class="n">StaticQuantConfig</span><span class="p">(</span><span class="n">w_dtype</span><span class="o">=</span><span class="s2">&quot;fp32&quot;</span><span class="p">,</span> <span class="n">act_dtype</span><span class="o">=</span><span class="s2">&quot;fp32&quot;</span><span class="p">))</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">fp32_model</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="n">run_fn</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
<ol class="simple">
<li><p>Example of <code class="docutils literal notranslate"><span class="pre">op_type_dict</span></code>
Here we don’t quantize <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># fallback by op_type</span>
<span class="n">quant_config</span><span class="o">.</span><span class="n">set_local</span><span class="p">(</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="n">StaticQuantConfig</span><span class="p">(</span><span class="n">w_dtype</span><span class="o">=</span><span class="s2">&quot;fp32&quot;</span><span class="p">,</span> <span class="n">act_dtype</span><span class="o">=</span><span class="s2">&quot;fp32&quot;</span><span class="p">))</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="n">run_fn</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-examples">
<h4>Model Examples<a class="headerlink" href="#model-examples" title="Link to this heading"></a></h4>
<p>Users could refer to <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/static_quant/ipex">examples</a> on how to quantize a new model.</p>
</section>
</section>
<section id="static-quantization-with-pt2e-backend">
<h3>Static Quantization with PT2E Backend<a class="headerlink" href="#static-quantization-with-pt2e-backend" title="Link to this heading"></a></h3>
<p>Compared to the IPEX backend, which uses JIT compilation to capture the eager model, the PT2E path uses <code class="docutils literal notranslate"><span class="pre">torch.dynamo</span></code> to capture the eager model into an FX graph model, and then inserts the observers and Q/QD pairs on it. Finally it uses the <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> to perform the pattern matching and replace the  Q/DQ pairs with optimized quantized operators.</p>
<section id="usage-sample-with-pt2e">
<h4>Usage Sample with PT2E<a class="headerlink" href="#usage-sample-with-pt2e" title="Link to this heading"></a></h4>
<p>There are four steps to perform W8A8 static quantization with PT2E backend: <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">prepare</span></code>, <code class="docutils literal notranslate"><span class="pre">convert</span></code> and <code class="docutils literal notranslate"><span class="pre">compile</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.torch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">StaticQuantConfig</span><span class="p">,</span> <span class="n">prepare</span><span class="p">,</span> <span class="n">convert</span>

<span class="c1"># Prepare the float model and example inputs for export model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">UserFloatModel</span><span class="p">()</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># Export eager model into FX graph model</span>
<span class="n">exported_model</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="c1"># Quantize the model</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">StaticQuantConfig</span><span class="p">()</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">exported_model</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>
<span class="c1"># Calibrate</span>
<span class="n">run_fn</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="c1"># Compile the quantized model and replace the Q/DQ pattern with Q-operator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._inductor</span><span class="w"> </span><span class="kn">import</span> <span class="n">config</span>

<span class="n">config</span><span class="o">.</span><span class="n">freezing</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">opt_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">q_model</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>Note: The <code class="docutils literal notranslate"><span class="pre">set_local</span></code> of <code class="docutils literal notranslate"><span class="pre">StaticQuantConfig</span></code> will be supported after the torch 2.4 release.</p>
</div></blockquote>
</section>
<section id="model-examples-with-pt2e">
<h4>Model Examples with PT2E<a class="headerlink" href="#model-examples-with-pt2e" title="Link to this heading"></a></h4>
<p>Users could refer to <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/examples/pytorch/cv/static_quant">cv examples</a> and <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/static_quant/pt2e">llm examples</a> on how to quantize a new model.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f76482a7230> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>