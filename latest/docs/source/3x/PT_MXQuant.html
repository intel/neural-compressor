

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Microscaling Quantization &mdash; Intel® Neural Compressor 3.7.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Microscaling Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/source/3x/PT_MXQuant.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="microscaling-quantization">
<h1>Microscaling Quantization<a class="headerlink" href="#microscaling-quantization" title="Link to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#get-start-with-microscaling-quantization-api">Get Started with Microscaling Quantization API</a></p></li>
<li><p><a class="reference external" href="#examples">Examples</a></p></li>
<li><p><a class="reference external" href="#reference">Reference</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Numerous breakthroughs have emerged across various fields, such as text analysis, language translation and chatbot technologies, fueled by the development of large language models (LLMs). Nevertheless, their increasing power comes with the challenge of explosive growth in parameters, posing obstacles for practical use. To balance memory limits and accuracy preservation for AI models, the Microscaling (MX) specification was promoted from the well-known Microsoft Floating Point (MSFP) data type [1, 2]:</p>
<table>
  <tr>
    <th>Format Name</th>
    <th>Element Data type</th>
    <th>Element Bits</th>
    <th>Scaling Block Size</th>
    <th>Scale Data Type</th> 
    <th>Scale Bits</th>
  </tr>
  <tr>
    <td rowspan="2">MXFP8</td>
    <td>FP8 (E5M2)</td>
    <td rowspan="2">8</td>
    <td rowspan="2">32</td>
    <td rowspan="2">E8M0</td>
    <td rowspan="2">8</td>
  </tr>
  <tr>
    <td>FP8 (E4M3)</td>
  </tr>
  <tr>
    <td rowspan="2">MXFP6</td>
    <td>FP6 (E3M2)</td>
    <td rowspan="2">6</td>
    <td rowspan="2">32</td>
    <td rowspan="2">E8M0</td>
    <td rowspan="2">8</td>
  </tr>
  <tr>
    <td>FP6 (E2M3)</td>
  </tr>
  <tr>
    <td>MXFP4</td>
    <td>FP4 (E2M1)</td>
    <td>4</td>
    <td>32</td>
    <td>E8M0</td> 
    <td>8</td>
  </tr>
  <tr>
    <td>MXINT8</td>
    <td>INT8</td>
    <td>8</td>
    <td>32</td>
    <td>E8M0</td> 
    <td>8</td>
  </tr>
</table><p>At an equivalent accuracy level, the MX data type demonstrates the ability to occupy a smaller area and incur lower energy costs for multiply-accumulate compared to other conventional data types on the same silicon [1].</p>
<p>Neural Compressor seamlessly applies the MX data type to post-training quantization, offering meticulously crafted recipes to empower users to quantize LLMs without sacrificing accuracy. The workflow is shown as below.</p>
<a target="_blank" href="./imgs/mx_workflow.png" text-align:left>
    <left> 
        <img src="./imgs/mx_workflow.png" alt="Workflow of MX Quant (source [3])" height=120> 
    </left>
</a><p>The memory and computational limits of LLMs are more severe than other general neural networks, so our exploration focuses on LLMs first. The following table shows the basic MX quantization recipes in Neural Compressor and enumerates distinctions among various data types. The MX data type replaces general float scale with powers of two to be more hardware-friendly.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th></th>
<th>MX Format</th>
<th>INT8</th>
<th>FP8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scale</td>
<td>$2^{exp}$</td>
<td>$\frac{MAX}{amax}$</td>
<td>$\frac{MAX}{amax}$</td>
</tr>
<tr>
<td>Zero point</td>
<td>0 (None)</td>
<td>$2^{bits - 1}$ or $-min * scale$</td>
<td>0 (None)</td>
</tr>
<tr>
<td>Granularity</td>
<td>per-block (default blocksize is 32)</td>
<td>per-channel or per-tensor</td>
<td>per-channel or per-tensor</td>
</tr>
</tbody>
</table><p>The exponent (exp) is equal to clamp(floor(log2(amax)) - maxExp, -127, 127), MAX is the representation range of the data type, amax is the max absolute value of per-block tensor, and rmin is the minimum value of the per-block tensor.</p>
</section>
<section id="get-started-with-microscaling-quantization-api">
<h2>Get Started with Microscaling Quantization API<a class="headerlink" href="#get-started-with-microscaling-quantization-api" title="Link to this heading"></a></h2>
<p>To get a model quantized with Microscaling Data Types, users can use the AutoRound Quantization API as follows.</p>
<section id="basic-usage">
<h3>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading"></a></h3>
<p>The following example demonstrates how to quantize a model using MX data types:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.torch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoRoundConfig</span><span class="p">,</span> <span class="n">prepare</span><span class="p">,</span> <span class="n">convert</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">fp32_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;./saved_inc&quot;</span>

<span class="c1"># quantization configuration</span>
<span class="c1"># `iters=0` means RTN (fast, no optimization); use default `iters=200` if accuracy is poor</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">AutoRoundConfig</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>  <span class="c1"># Tokenizer for processing calibration data</span>
    <span class="n">iters</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Number of optimization iterations (default: 200)</span>
    <span class="n">scheme</span><span class="o">=</span><span class="s2">&quot;MXFP4&quot;</span><span class="p">,</span>  <span class="c1"># MX quantization scheme: &quot;MXFP4&quot;, &quot;MXFP8&quot;</span>
    <span class="n">export_format</span><span class="o">=</span><span class="s2">&quot;auto_round&quot;</span><span class="p">,</span>  <span class="c1"># Export format for the quantized model</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>  <span class="c1"># Directory to save the quantized model (default: &quot;temp_auto_round&quot;)</span>
<span class="p">)</span>

<span class="c1"># quantize the model and save to output_dir</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">fp32_model</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># loading</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

<span class="c1"># inference</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;There is a girl who likes adventure,&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</section>
<section id="advantages-of-mx-quantization">
<h3>Advantages of MX Quantization<a class="headerlink" href="#advantages-of-mx-quantization" title="Link to this heading"></a></h3>
<ol class="simple">
<li><p><strong>Hardware-Friendly</strong>: Uses power-of-2 scaling factors for efficient hardware implementation</p></li>
<li><p><strong>Fine-Grained Quantization</strong>: Per-block scaling (block size = 32) provides better accuracy than per-tensor or per-channel methods</p></li>
<li><p><strong>Zero-Point Free</strong>: No zero-point overhead, simplifying computation</p></li>
<li><p><strong>Memory Efficient</strong>: Significantly reduces model size while maintaining competitive accuracy</p></li>
<li><p><strong>Energy Efficient</strong>: Lower energy consumption for multiply-accumulate operations compared to traditional data types</p></li>
</ol>
</section>
</section>
<section id="mix-precision-mxfp4-mxfp8">
<h2>Mix Precision (MXFP4 + MXFP8)<a class="headerlink" href="#mix-precision-mxfp4-mxfp8" title="Link to this heading"></a></h2>
<p>To achieve optimal compression ratios with acceptable accuracy, we integrate AutoRound automatic mix-precision algorithm. The mix-precision approach combines MXFP4 and MXFP8 formats to quantize different layers of the model based on their sensitivity to quantization.</p>
<section id="benefits-of-mix-precision">
<h3>Benefits of Mix Precision<a class="headerlink" href="#benefits-of-mix-precision" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Better Accuracy-Compression Trade-off</strong>: Sensitive layers use MXFP8 (higher precision) while less sensitive layers use MXFP4 (higher compression), optimizing the overall model performance.</p></li>
<li><p><strong>Flexible Configuration</strong>: Users can customize the precision assignment strategy based on their specific accuracy and compression requirements.</p></li>
<li><p><strong>Automatic Layer Selection</strong>: The AutoRound algorithm automatically identifies which layers should use which precision level, reducing manual tuning effort.</p></li>
</ul>
</section>
<section id="target-bits-configuration">
<h3>Target Bits Configuration<a class="headerlink" href="#target-bits-configuration" title="Link to this heading"></a></h3>
<p>To achieve optimal compression ratios in mixed-precision quantization, we provide the <code class="docutils literal notranslate"><span class="pre">target_bits</span></code> parameter for automated precision configuration.</p>
<ul class="simple">
<li><p><strong>Single target bit</strong>: If you pass a single float number, it will automatically generate an optimal quantization recipe to achieve that target average bit-width.</p></li>
<li><p><strong>Multiple target bits</strong>: If you pass multiple float numbers, it will generate multiple recipes for different target bit-widths, allowing you to compare trade-offs between model size and accuracy.</p></li>
</ul>
<p><strong>Note</strong>: For MX data type, <code class="docutils literal notranslate"><span class="pre">target_bits</span></code> ranges from 4.25 to 8.25 due to scale bits overhead.</p>
</section>
<section id="usage-example">
<h3>Usage Example<a class="headerlink" href="#usage-example" title="Link to this heading"></a></h3>
<section id="autotune-with-multiple-target-bits">
<h4>AutoTune with Multiple Target Bits<a class="headerlink" href="#autotune-with-multiple-target-bits" title="Link to this heading"></a></h4>
<p>For automatically finding the best configuration across multiple target bits:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">neural_compressor.torch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoRoundConfig</span><span class="p">,</span> <span class="n">autotune</span><span class="p">,</span> <span class="n">TuningConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">fp32_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span><span class="p">)</span>


<span class="c1"># Define evaluation function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_fn</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># Implement your evaluation logic here</span>
    <span class="c1"># Return accuracy score</span>
    <span class="k">pass</span>


<span class="c1"># Configuration with multiple target bits</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoRoundConfig</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">nsamples</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">seqlen</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">iters</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">target_bits</span><span class="o">=</span><span class="p">[</span><span class="mf">7.2</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">],</span>  <span class="c1"># Try multiple target bits</span>
    <span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;MXFP4&quot;</span><span class="p">,</span> <span class="s2">&quot;MXFP8&quot;</span><span class="p">],</span>
    <span class="n">shared_layers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">[</span><span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;q_proj&quot;</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">],</span>
    <span class="p">],</span>
    <span class="n">export_format</span><span class="o">=</span><span class="s2">&quot;auto_round&quot;</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./llama3.1-8B-MXFP4-MXFP8&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># AutoTune to find the best configuration</span>
<span class="n">tuning_config</span> <span class="o">=</span> <span class="n">TuningConfig</span><span class="p">(</span><span class="n">config_set</span><span class="o">=</span><span class="p">[</span><span class="n">config</span><span class="p">],</span> <span class="n">tolerable_loss</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">autotune</span><span class="p">(</span><span class="n">fp32_model</span><span class="p">,</span> <span class="n">tuning_config</span><span class="p">,</span> <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="key-parameters-for-mix-precision">
<h3>Key Parameters for Mix Precision<a class="headerlink" href="#key-parameters-for-mix-precision" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>target_bits</strong>: Target average bit-width for the model. Can be a single float or a list of floats.</p>
<ul>
<li><p>Single value: Generates one recipe for that specific target bit-width</p></li>
<li><p>Multiple values: Generates multiple recipes for comparison and selects the best one via autotune</p></li>
</ul>
</li>
<li><p><strong>options</strong>: List of available data types for mixed precision (e.g., <code class="docutils literal notranslate"><span class="pre">[&quot;MXFP4&quot;,</span> <span class="pre">&quot;MXFP8&quot;]</span></code>)</p></li>
<li><p><strong>shared_layers</strong>: List of layer groups that should use the same precision. Each group is a list of layer name patterns.</p>
<ul>
<li><p>Ensures architectural consistency (e.g., all attention projections use the same precision)</p></li>
<li><p>Improves model performance by maintaining balanced computation</p></li>
</ul>
</li>
<li><p><strong>tolerable_loss</strong>: Maximum acceptable accuracy loss compared to FP32 baseline (used with autotune)</p></li>
</ul>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<section id="pytorch-examples">
<h3>PyTorch Examples<a class="headerlink" href="#pytorch-examples" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Multimodal Models</strong>: <a class="reference external" href="/examples/pytorch/multimodal-modeling/quantization/auto_round/llama4">Llama-4-Scout-17B-16E-Instruct with MXFP4</a></p></li>
<li><p><strong>Language Models</strong>: <a class="reference external" href="/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/auto_round/llama3">Llama3 series with MXFP4/MXFP8 and Mix Precision</a></p>
<ul>
<li><p>Llama 3.1 8B: MXFP8, MXFP4, and Mix Precision (target_bits=7.8)</p></li>
<li><p>Llama 3.3 70B: MXFP8, MXFP4, and Mix Precision (target_bits=5.8)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="best-practices-and-tips">
<h2>Best Practices and Tips<a class="headerlink" href="#best-practices-and-tips" title="Link to this heading"></a></h2>
<section id="choosing-the-right-data-type">
<h3>Choosing the Right Data Type<a class="headerlink" href="#choosing-the-right-data-type" title="Link to this heading"></a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Data Type</th>
<th>Compression</th>
<th>Accuracy</th>
<th>Use Case</th>
<th>Export Format</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MXFP8</strong></td>
<td>Moderate (8-bit)</td>
<td>High</td>
<td>Production models where accuracy is critical</td>
<td><code>auto_round</code></td>
</tr>
<tr>
<td><strong>MXFP4</strong></td>
<td>High (4-bit)</td>
<td>Moderate</td>
<td>Aggressive compression with acceptable accuracy loss</td>
<td><code>auto_round</code></td>
</tr>
<tr>
<td><strong>MXFP4+MXFP8 Mix</strong></td>
<td>Configurable (4.25-8.25 bits)</td>
<td>High</td>
<td>Best balance between compression and accuracy</td>
<td><code>auto_round</code></td>
</tr>
</tbody>
</table></section>
<section id="common-issues-and-solutions">
<h3>Common Issues and Solutions<a class="headerlink" href="#common-issues-and-solutions" title="Link to this heading"></a></h3>
<p><strong>Issue</strong>: Out of Memory (OOM) during quantization</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Use <code class="docutils literal notranslate"><span class="pre">low_gpu_mem_usage=True</span></code>, enable <code class="docutils literal notranslate"><span class="pre">enable_torch_compile</span></code>, reduce <code class="docutils literal notranslate"><span class="pre">nsamples</span></code>, or use smaller <code class="docutils literal notranslate"><span class="pre">seqlen</span></code></p></li>
</ul>
<p><strong>Issue</strong>: Accuracy drop is too large</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Increase <code class="docutils literal notranslate"><span class="pre">iters</span></code>, use more <code class="docutils literal notranslate"><span class="pre">nsamples</span></code>, or try mixed precision with higher <code class="docutils literal notranslate"><span class="pre">target_bits</span></code></p></li>
</ul>
<p><strong>Issue</strong>: Quantization is too slow</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Reduce <code class="docutils literal notranslate"><span class="pre">iters</span></code> or set to 0 for RTN, decrease <code class="docutils literal notranslate"><span class="pre">nsamples</span></code>, enable <code class="docutils literal notranslate"><span class="pre">enable_torch_compile</span></code></p></li>
</ul>
<p><strong>Issue</strong>: Model loading fails after quantization</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Refer to <a class="reference external" href="/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/auto_round/llama3/README.html#inference">auto_round/llama3/inference</a></p></li>
</ul>
</section>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<p>[1]: Darvish Rouhani, Bita, et al. “Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point.” Advances in neural information processing systems 33 (2020): 10271-10281</p>
<p>[2]: OCP Microscaling Formats (MX) Specification</p>
<p>[3]: Rouhani, Bita Darvish, et al. “Microscaling Data Formats for Deep Learning.” arXiv preprint arXiv:2310.10537 (2023).</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f89d145bb00> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>