

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Full Publications/Events (91) &mdash; Intel® Neural Compressor 3.8 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Full Publications/Events (91)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/source/publication_list.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="full-publications-events-91">
<h1>Full Publications/Events (91)<a class="headerlink" href="#full-publications-events-91" title="Link to this heading"></a></h1>
<section id="id1">
<h2>2025 (5)<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-qwen3-large-language-models.html">Intel® AI Solutions Accelerate Qwen3 Large Language Models</a> (May 2025)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/llama4-performance-on-intel-hardware.html">Intel® Data Center Al Solutions and Llama 4 Herd Performance Insights</a> (Apr 2025)</p></li>
<li><p>arXiv: <a class="reference external" href="https://arxiv.org/abs/2503.09975">Faster Inference of LLMs using FP8 on the Intel Gaudi</a> (Mar 2025)</p></li>
<li><p>PyTorch landscape: <a class="reference external" href="https://landscape.pytorch.org/">PyTorch general optimizations</a> (Mar 2025)</p></li>
<li><p>Blog on SqueezeBits: <a class="reference external" href="https://blog.squeezebits.com/intel-gaudi-4-fp8-quantization--40269">[Intel Gaudi] #4. FP8 Quantization</a> (Jan 2025)</p></li>
</ul>
</section>
<section id="id2">
<h2>2024 (7)<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Blog by Microsoft: <a class="reference external" href="https://techcommunity.microsoft.com/blog/machinelearningblog/phi-4-quantization-and-inference-speedup/4360047">Phi-4 quantization and inference speedup</a> (Dec 2024)</p></li>
<li><p>EMNLP’2024: <a class="reference external" href="https://arxiv.org/abs/2309.05516">Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a> (Sep 2024)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/intel-neural-compressor-v3-0-a-quantization-tool-across-intel-hardware-9856adee6f11">Quantization on Intel Gaudi Series AI Accelerators</a> (Aug 2024)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/accelerating-qwen2-models-with-intel-extension-for-transformers-99403de82f68">Accelerating Qwen2 Models with Intel Extension for Transformers</a> (June 2024)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Neural-Compressor-Boosting-AI-Model-Efficiency/post/1604740">Neural Compressor: Boosting AI Model Efficiency</a> (June 2024)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-solutions-accelerate-alibaba-qwen2-llms.html">Optimization of Intel AI Solutions for Alibaba Cloud’s Qwen2 Large Language Models</a> (June 2024)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-meta-llama3-with-intel-ai-solutions.html">Accelerate Meta* Llama 3 with Intel AI Solutions</a> (Apr 2024)</p></li>
</ul>
</section>
<section id="id3">
<h2>2023 (25)<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Blog by Intel: <a class="reference external" href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Effective-Weight-Only-Quantization-for-Large-Language-Models/post/1529552">Effective Weight-Only Quantization for Large Language Models with Intel® Neural Compressor</a> (Oct 2023)</p></li>
<li><p>arXiv: <a class="reference external" href="https://arxiv.org/abs/2309.14592">Efficient Post-training Quantization with FP8 Formats</a> (Sep 2023)</p></li>
<li><p>EMNLP’2023 (Under Review): <a class="reference external" href="https://openreview.net/forum?id=iaI8xEINAf&amp;referrer=%5BAuthor%20Console%5D">TEQ: Trainable Equivalent Transformation for Quantization of LLMs</a> (Sep 2023)</p></li>
<li><p>arXiv: <a class="reference external" href="https://arxiv.org/abs/2309.05516">Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a> (Sep 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;NeuralCompressor/quantization-accuracy-loss-diagnosis-with-neural-insights-5d73f4ca2601">Quantization Accuracy Loss Diagnosis with Neural Insights</a> (Aug 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/faster-stable-diffusion-inference-with-intel-extension-for-transformers-on-intel-platforms-7e0f563186b0">Faster Stable Diffusion Inference with Intel Extension for Transformers</a> (July 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://www.youtube.com/watch?v=luYBWA1Q5pQ">ONNXCommunityMeetup2023: INT8 Quantization for Large Language Models with Intel Neural Compressor</a> (July 2023)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html">Accelerate Llama 2 with Intel AI Hardware and Software Optimizations</a> (July 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;NeuralCompressor/model-quantization-diagnosis-with-neural-insights-8117033fba43">Model quantization diagnosis with Neural Insights</a> (July 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/simplify-your-custom-chatbot-deployment-on-intel-platforms-c8a911d906cf">Simplify Your Custom Chatbot Deployment</a> (June 2023)</p></li>
<li><p>Blog by MSFT: <a class="reference external" href="https://cloudblogs.microsoft.com/opensource/2023/06/26/olive-a-user-friendly-toolchain-for-hardware-aware-model-optimization/">Olive: A user-friendly toolchain for hardware-aware model optimization</a> (June 2023)</p></li>
<li><p>Blog by MSFT: <a class="reference external" href="https://cloudblogs.microsoft.com/opensource/2023/06/26/automate-optimization-techniques-for-transformer-models/">Automate optimization techniques for transformer models</a> (June 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://www.youtube.com/watch?v=5xHKe4wWLes&amp;list=PLg-UKERBljNxC8dmjx7jJA2YADWOFuj_p&amp;index=4">Get Started Post-Training Dynamic Quantization | AI Model Optimization with Intel® Neural Compressor</a> (June 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://www.youtube.com/watch?v=ie3w_j0Ntsk">How to Choose AI Model Quantization Techniques | AI Model Optimization with Intel® Neural Compressor</a> (June 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://www.youtube.com/watch?v=m2LokuUdeVg&amp;list=PLg-UKERBljNxC8dmjx7jJA2YADWOFuj_p&amp;index=2">What is AI Model Optimization | AI Model Optimization with Intel® Neural Compressor | Intel Software</a> (June 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/streamlining-model-optimization-as-a-service-with-intel-neural-compressor-fd970fdb2928">Streamlining Model Optimization as a Service with Intel Neural Compressor</a> (June 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;amerather_9719/intel-optimization-at-netflix-79ef0efb9d2">Intel Optimization at Netflix</a> (May 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;NeuralCompressor/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98">Effective Post-training Quantization for Large Language Models with Enhanced SmoothQuant Approach</a> (Apr 2023)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-Processors-Are-Still-the-Only-CPU-With-MLPerf-Results/post/1472750">Intel® Xeon® Processors Are Still the Only CPU With MLPerf Results, Raising the Bar By 5x</a> (Apr 2023)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/neural-compressor-tencent-cloud-taco-kit-ai.html">集成英特尔® Neural Compressor，腾讯云TACO Kit为AI应用带来高效异构加速服务</a> (Mar 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://mp.weixin.qq.com/s/I-FQqOuW7HTnwXegLGNAtw">Adopt with Tencent TACO: Heterogeneous optimization is also key to improving AI computing power</a> (Mar 2023)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/structured-pruning-for-transformer-based-models-116e949ef12c">Structured Pruning for Transformer-Based Models</a> (Jan 2023)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://www.youtube.com/watch?v=emCgSTlJaAg">Training and Inference for Stable Diffusion | Intel Business</a> (Jan 2023)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/alibaba-solution-brief.html">Intel® AMX Enhances AI Inference Performance</a> (Jan 2023)</p></li>
<li><p>Blog by TensorFlow: <a class="reference external" href="https://blog.tensorflow.org/2023/01/optimizing-tensorflow-for-4th-gen-intel-xeon-processors.html">Optimizing TensorFlow for 4th Gen Intel Xeon Processors</a> (Jan 2023)</p></li>
</ul>
</section>
<section id="id4">
<h2>2022 (35)<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;NeuralCompressor/from-innovation-to-ecosystem-a-journey-of-intel-neural-compressor-aa61530a9098">From Innovation to Ecosystem: A Journey of Intel Neural Compressor</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;kawapanion/mlefficiency-optimizing-transformer-models-for-efficiency-a9e230cff051">MLefficiency — Optimizing transformer models for efficiency</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/one-click-acceleration-of-huggingface-transformers-with-optimum-intel-by-neural-coder-f35ca3b1a82f">One-Click Acceleration of Hugging Face Transformers with Intel’s Neural Coder</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/one-click-quantize-your-deep-learning-code-in-visual-studio-code-with-neural-coder-extension-8be1a0022c29">One-Click Quantization of Deep Learning Models with the Neural Coder Extension</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/accelerating-stable-diffusion-inference-through-8-bit-post-training-quantization-with-intel-neural-e28f3615f77c">Accelerate Stable Diffusion with Intel Neural Compressor</a> (Dec 2022)</p></li>
<li><p>Blog on WeChat: <a class="reference external" href="https://mp.weixin.qq.com/s/CPz9-5Nsh-5N9Q8-UmK--w">Intel together with Tencent deepens the cooperation to build a cloud foundation for digital and intelligent industry</a> (Dec 2022)</p></li>
<li><p>Blog on VMware: <a class="reference external" href="https://marketplace.cloud.vmware.com/services/details/e9c3d891-ca51-4f07-a5aa-3fe6394f15ae">Intel Neural Compressor for TF Virtual Appliance packaged by Bitnami</a> (Nov 2022)</p></li>
<li><p>Blog on Tencent Cloud: <a class="reference external" href="https://cloud.tencent.com/developer/article/2165895">Neural Compressor: an open-source Python library for network compression</a> (Nov 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/syncedreview/running-fast-transformers-on-cpus-intel-approach-achieves-significant-speed-ups-and-sota-448521704c5e">Running Fast Transformers on CPUs: Intel Approach Achieves Significant Speed Ups and SOTA Performance</a> (Nov 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/personalized-stable-diffusion-with-few-shot-fine-tuning-on-a-single-cpu-f01a3316b13">Personalized Stable Diffusion with Few-Shot Fine-Tuning</a> (Nov 2022)</p></li>
<li><p>NeurIPS’2022: <a class="reference external" href="https://arxiv.org/abs/2211.07715">Fast Distilbert on CPUs</a> (Oct 2022)</p></li>
<li><p>NeurIPS’2022: <a class="reference external" href="https://arxiv.org/abs/2210.17114">QuaLA-MiniLM: a Quantized Length Adaptive MiniLM</a> (Oct 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/innovation-of-ai-software-extension-tensorflow.html">Meet the Innovation of Intel AI Software: Intel® Extension for TensorFlow*</a> (Oct 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-inference-with-intel-neural-compressor.html#gs.gnq0cj">PyTorch* Inference Acceleration with Intel® Neural Compressor</a> (Oct 2022)</p></li>
<li><p>Post on Social Media: Neural Coder, a new plug-in for Intel Neural Compressor was covered by <a class="reference external" href="https://twitter.com/IntelDevTools/status/1583629213697212416">Twitter</a>, <a class="reference external" href="https://www.linkedin.com/posts/intel-software_oneapi-ai-deeplearning-activity-6989377309917007872-Dbzg?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>, and <a class="reference external" href="https://mp.weixin.qq.com/s/LL-4eD-R0YagFgODM23oQA">Intel Developer Zone</a> from Intel, and <a class="reference external" href="https://twitter.com/IntelDevTools/status/1583629213697212416/retweets">Twitter</a> and <a class="reference external" href="https://www.linkedin.com/feed/update/urn:li:share:6990377841435574272/">LinkedIn</a> from Hugging Face. (Oct 2022)</p></li>
<li><p>Marketplace Distribute: Intel Neural Compressor successfully landed on <a class="reference external" href="https://console.cloud.google.com/marketplace/product/bitnami-launchpad/inc-tensorflow-intel?project=verdant-sensor-286207">GCP</a>, <a class="reference external" href="https://aws.amazon.com/marketplace/pp/prodview-yjyh2xmggbmga#pdp-support">AWS</a>, and <a class="reference external" href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bitnami.inc-tensorflow-intel">Azure</a> marketplace. (Oct 2022)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://twitter.com/i/status/1574909338203967497">Neural Coder (Intel Neural Compressor Plug-in): One-Click, No-Code Solution (Pat’s Keynote IntelON 2022)</a> (Sep 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/alibaba-cloud-collaborates-with-intel-neural-compressor-for-better-productivity-and-performance-83cdb6500420">Alibaba Cloud and Intel Neural Compressor Deliver Better Productivity for PyTorch Users</a> [<a class="reference external" href="https://mp.weixin.qq.com/s/LL-4eD-R0YagFgODM23oQA">Chinese version</a>] (Sep 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/efficient-text-classification-with-intel-neural-compressor-4853296deeac">Efficient Text Classification with Intel Neural Compressor</a> (Sep 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/dynamic-neural-architecture-search-with-intel-neural-compressor-7b05eaf325f3">Dynamic Neural Architecture Search with Intel Neural Compressor</a> (Sep 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/easy-quantization-in-pytorch-using-fine-grained-fx-80be2c4bc2d6">Easy Quantization in PyTorch Using Fine-Grained FX</a> (Sep 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/one-click-enable-intel-neural-compressor-features-in-pytorch-scripts-5d4e31f5a22b">One-Click Enabling of Intel Neural Compressor Features in PyTorch Scripts</a> (Aug 2022)</p></li>
<li><p>Blog by Alibaba: <a class="reference external" href="https://zhuanlan.zhihu.com/p/552484413?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=667097517833981952">Deep learning inference optimization for Address Purification</a> (Aug 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/videos/accelerate-inference-without-sacrificing-accuracy.html#gs.9yottx">Accelerate AI Inference without Sacrificing Accuracy</a> (Jun 2022)</p></li>
<li><p>Blog by Meta: <a class="reference external" href="https://medium.com/pytorch/pytorch-inference-acceleration-with-intel-neural-compressor-842ef4210d7d">PyTorch Inference Acceleration with Intel® Neural Compressor</a> (Jun 2022)</p></li>
<li><p>Blog by Hugging Face: <a class="reference external" href="https://huggingface.co/blog/intel">Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration</a> (Jun 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html">Intel® Neural Compressor oneAPI</a> (Jun 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://networkbuilders.intel.com/solutionslibrary/intel-deep-learning-boost-boost-network-security-ai-inference-performance-in-google-cloud-platform-gcp-technology-guide">Intel® Deep Learning Boost - Boost Network Security AI Inference Performance in Google Cloud Platform (GCP)</a> (Apr 2022)</p></li>
<li><p>PyTorch Ecosystem: <a class="reference external" href="https://pytorch.org/ecosystem/">INC as PT ecosystem project</a> (Apr 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://builders.intel.com/docs/networkbuilders/ai-technologies-unleash-ai-innovation-in-network-applications-solution-brief-1637303210.pdf">New instructions in the Intel® Xeon® Scalable processors combined with optimized software frameworks enable real-time AI within network workloads</a> (Feb 2022)</p></li>
<li><p>Joint blog with MSFT: <a class="reference external" href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Quantizing-ONNX-Models-using-Intel-Neural-Compressor/post/1355237">Quantizing ONNX Models using Intel® Neural Compressor</a> (Feb 2022)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/quantize-ai-by-oneapi-analytics-on-alibaba-cloud.html">Quantize AI Model by Intel® oneAPI AI Analytics Toolkit on Alibaba Cloud</a> (Feb 2022)</p></li>
<li><p>Blog by SigOpt: <a class="reference external" href="https://sigopt.com/blog/intel-neural-compressor-quantization-with-sigopt/">Intel Neural Compressor Quantization with SigOpt</a> (Jan 2022)</p></li>
<li><p>Post on Social Media: <a class="reference external" href="https://twitter.com/IntelAI/status/1469079414562557952">AI Performance and Productivity with Intel® Neural Compressor</a> (Jan 2022)</p></li>
<li><p>PyTorch Ecosystem: <a class="reference external" href="https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with Intel® Neural Compressor</a> (Jan 2022)</p></li>
</ul>
</section>
<section id="id5">
<h2>2021 (15)<a class="headerlink" href="#id5" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Tutorial on BiliBili: <a class="reference external" href="https://space.bilibili.com/1840724569?from=search&amp;seid=8673550305007703901&amp;spm_id_from=333.337.0.0">Intel Neural Compressor Tutorial on BiliBili</a> (Dec 2021)</p></li>
<li><p>Blog on GESTALT IT: <a class="reference external" href="https://gestaltit.com/tech-talks/intel/intel-2021/jpwarren/faster-ai-ml-results-with-intel-neural-compressor">Faster AI/ML Results With Intel Neural Compressor</a> (Dec 2021)</p></li>
<li><p>AI Submit’21: <a class="reference external" href="https://www.youtube.com/watch?v=-_2ha2CNWXA">Dynamic Quantization with Intel Neural Compressor and Transformers</a> (Nov 2021)</p></li>
<li><p>NeurIPS’21: <a class="reference external" href="https://nips.cc/Conferences/2021/Schedule?showEvent=21839">Prune Once for All: Sparse Pre-Trained Language Models</a> (Nov 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/optimization-with-intel-neural-compressor.html">Faster, Easier Optimization with Intel® Neural Compressor</a> (Nov 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/videos/accelerate-deep-learning-with-intel-tensorflow.html#gs.9yrw90">Accelerate Deep Learning with Intel® Extension for TensorFlow*</a> (Oct 2021)</p></li>
<li><p>Post on Social Media: Intel® Neural Compressor: A Scalable Quantization Tool for ONNX Models post on <a class="reference external" href="https://youtu.be/Irk9UIcsCng">YouTube</a> and <a class="reference external" href="https://twitter.com/onnxai/status/1465376442066227205?s=20">Twitter</a> (Oct 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/intel-mlperf-inference-performance.html">A “Double Play” for MLPerf™ Inference Performance Gains with 3rd Generation Intel® Xeon® Scalable Processors</a> (Sep 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/articles/optimize-tensorflow-pre-trained-model-inference.html">Optimize TensorFlow Pre-trained Model for Inference</a> (Jun 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/tencent-3d-digital-face-reconstruction.html">3D Digital Face Reconstruction Solution enabled by 3rd Gen Intel® Xeon® Scalable Processors</a> (Apr 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/alibaba-lpot.html">Accelerating Alibaba Transformer model performance with 3rd Gen Intel® Xeon® Scalable Processors (Ice Lake) and Intel® Deep Learning Boost</a> (Apr 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/3rd-gen-xeon-mlperf-performance-gains.html">MLPerf™ Performance Gains Abound with latest 3rd Generation Intel® Xeon® Scalable Processors</a> (Apr 2021)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://techdecoded.intel.io/essentials/using-low-precision-optimizations-for-high-performance-dl-inference-applications/#gs.z20k91">Using Low-Precision Optimizations for High-Performance DL Inference Applications</a> (Apr 2021)</p></li>
<li><p>ONNX Ecosystem: <a class="reference external" href="https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=35160391">Quantization support for ONNX using LPOT (Low precision optimization tool)</a> (Mar 2021)</p></li>
<li><p>Blog on NextPlatform:<a class="reference external" href="https://www.nextplatform.com/2021/02/01/cern-uses-dlboost-oneapi-to-juice-inference-without-accuracy-loss/">DL Boost Quantization with CERN’s 3D-GANs model</a> (Feb 2021)</p></li>
</ul>
</section>
<section id="id6">
<h2>2018 - 2020 (4)<a class="headerlink" href="#id6" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Joint presentation with CERN: <a class="reference external" href="https://indico.cern.ch/event/852553/contributions/4059283/attachments/2126838/3581708/Rehm_Florian-IML-Reduced_Precision.pdf">Reduced Precision Strategies for Deep Learning: 3DGAN Use Case</a> - <a class="reference external" href="https://indico.cern.ch/event/852553/contributions/4059283/attachments/2126838/3588271/IML2020_wedam_rehm.mp4">presentation</a> on <a class="reference external" href="https://indico.cern.ch/event/852553/contributions/4059283/">4th IML Machine Learning Workshop</a> (Oct 2020)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/intel-low-precision-optimization-tool.html">Intel Neural Compressor</a> (Sep 2020)</p></li>
<li><p>Blog by Intel: <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/lower-numerical-precision-deep-learning-inference-and-training.html">Lower Numerical Precision Deep Learning Inference and Training</a> (May 2018)</p></li>
<li><p>ASPLOS’18: <a class="reference external" href="https://arxiv.org/abs/1805.08691">Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe</a> (May 2018)</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fe20a42d6d0> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>