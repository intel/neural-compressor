## Copyright (c) 2021 Intel Corporation
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##    http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
##
#

-
  version:
    name: '1.6.0'
  int8: &ref_1_6 {
    'static': &ref_1_6_static {
      'Conv': {
        'weight':   &int8_sym_perchanneltensor_minmax {
                    'dtype': ['int8'],
                    'scheme': ['sym'],
                    'granularity': ['per_channel', 'per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': &uint8_asym_pertensor_minmax {
                    'dtype': ['uint8'],
                    'scheme': ['asym'],
                    'granularity': ['per_tensor'],
                    'algorithm': ['minmax']
                    },
        'mode': ['QDQ', 'QLinear']
      },
      'FusedConv': {
        'weight':   *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   &uint8_asym_perchanneltensor_minmax {
                    'dtype': ['uint8'],
                    'scheme': ['asym'],
                    'granularity': ['per_channel', 'per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   &int8_sym_pertensor_minmax {
                    'dtype': ['int8'],
                    'scheme': ['sym'],
                    'granularity': ['per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': &default_static_qlinear_qdq {
        'weight':   *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Mul': &default_static_qlinear {
        'weight':   *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QLinear']
      },
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
    },
    'dynamic': &ref_1_6_dynamic {
      'Conv': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': &default_dynamic {
        'weight': *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'EmbedLayerNormalization': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  recipes: &default_optimization
    graph_optimization:   # from onnxruntime graph_optimization_level
      level: ['DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL']

-
  version:
    name: '1.7.0'
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': *default_static_qlinear_qdq,
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
    },
    'dynamic': *ref_1_6_dynamic
  }
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.8.0'
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':  *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': {
      'Conv': {
        'weight': *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight': *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': {
        'weight': *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'EmbedLayerNormalization': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.9.0'
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': &ref_1_9_dynamic {
      'Conv': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': {
        'weight': *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.10.0'
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': *ref_1_9_dynamic
  }
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.11.0'
  int8: &ref_1_11 {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gemm': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'ArgMax': *default_static_qlinear,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': *ref_1_9_dynamic
  }
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.14.0'
  int8: *ref_1_11
  bf16: &common_bf16 ['MatMul', 'Gemm', 'BatchNormalization', 'Softmax', 'Sum',
    'Abs', 'BiasGelu', 'Exp', 'FastGelu', 'Gelu', 'Log', 'Relu', 'Round', 'Sigmoid',
    'Sqrt', 'Tanh', 'Add', 'Sub', 'Mul', 'Div', 'Pow', 'ReduceMean', 'Equal',
    'FusedMatMul', 'Greater', 'GreaterOrEqual', 'LeakyRelu', 'Less', 'LessOrEqual',
    'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze', 'ReduceL1', 'ReduceL2', 'ReduceLogSum',
    'ReduceLogSumExp', 'ReduceMax', 'ReduceProd', 'ReduceSum', 'ReduceSumSquare',
    'LayerNormalization', 'Concat']
  recipes:
    <<: *default_optimization

-
  version:
    name: 'default'
  int8: *ref_1_6
  recipes:
    <<: *default_optimization
