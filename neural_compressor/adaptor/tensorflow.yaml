#
# Copyright (c) 2022 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
-
  version:
    name: ['2.11.0202242', '2.11.0202250']

  precisions:
    names: int8, uint8, bf16, fp32
    valid_mixed_precisions: []

  ops:
    int8: ['Conv2D', 'Conv3D', 'DepthwiseConv2dNative', 'FusedBatchNorm', 'FusedBatchNormV2','FusedBatchNormV3',
           'MatMul', 'BatchMatMul', 'BatchMatMulV2', 'ConcatV2', 'MaxPool', 'MaxPool3D', 'AvgPool', '_MklFusedInstanceNorm',
           'Conv2DBackpropInput', 'Conv3DBackpropInputV2']
    uint8: ['Conv2D', 'Conv3D', 'DepthwiseConv2dNative', 'MatMul', 'BatchMatMul', 'BatchMatMulV2', 'ConcatV2',
            'MaxPool', 'MaxPool3D', 'AvgPool', 'Conv2DBackpropInput', 'Conv3DBackpropInputV2']
    bf16: ["_MklLayerNorm", "Conv2D", "Conv2DBackpropFilter", "Conv2DBackpropInput", "Conv3D", "Conv3DBackpropFilterV2", "Conv3DBackpropInputV2",
           "DepthwiseConv2dNative", "DepthwiseConv2dNativeBackpropFilter", "DepthwiseConv2dNativeBackpropInput", "GRUBlockCell",
           "AUGRUBlockCell", "MklGRU", "MklAUGRU", "MatMul", "BatchMatMul", "BatchMatMulV2", "_MklFusedBatchMatMulV2", "Einsum", # allow_list
           "Add", "AddN", "AddV2", "AvgPool", "AvgPool3D", "AvgPool3DGrad", "AvgPoolGrad", "BiasAdd", "BiasAddGrad", "BiasAddV1",
           "Erf", "FusedBatchNormV2", "FusedBatchNormGradV2", "FusedBatchNormV3", "FusedBatchNormGradV3", "LeakyRelu", "LeakyReluGrad",
           "Mean", "Mul", "Sub", "Elu", "EluGrad", "FloorDiv", "_FusedBatchNormEx", "Log", "Log1p", "LogSoftmax", "Prod", "RealDiv",
           "Reciprocal", "Rsqrt", "Selu", "SeluGrad", "Sigmoid", "SigmoidGrad", "Softmax", "Softplus", "SoftplusGrad", "Softsign",
           "SoftsignGrad", "Sqrt", "Square", "SquaredDifference", "Sum", "Tanh", "TanhGrad", "SparseSegmentSqrtN", # infer_list
           "Abs", "ArgMax","ArgMin","BatchToSpace","BatchToSpaceND","BroadcastTo","Ceil","CheckNumerics","ClipByValue","Concat","ConcatV2",
           "DepthToSpace","DynamicPartition","DynamicStitch","EnsureShape","Enter","Equal","Exit","ExpandDims","Fill","Floor","Gather",
           "GatherNd","GatherV2","Greater","GreaterEqual","Identity","IsFinite","IsInf","IsNan","Less","LessEqual","Max","Maximum","MaxPool",
           "MaxPool3D","MaxPool3DGrad","MaxPoolGrad","MaxPoolGradGrad","MaxPoolGradGradV2","MaxPoolGradV2","MaxPoolV2","Merge","Min","Minimum",
           "MirrorPad","MirrorPadGrad","Neg","NextIteration","NotEqual","OnesLike","Pack","Pad","PadV2","PreventGradient","Rank","Relu","Relu6",
           "Relu6Grad","ReluGrad","Reshape","ResizeNearestNeighbor","ResizeNearestNeighborGrad", "ResizeBilinear", "Reverse","ReverseSequence",
           "ReverseV2","Round", "Select","SelectV2","Shape","ShapeN","Sign","Slice","Snapshot","SpaceToBatch","SpaceToBatchND","SpaceToDepth",
           "Split","SplitV","Squeeze","StopGradient","StridedSlice","StridedSliceGrad","Switch","Tile","TopK","TopKV2","Transpose",
           "Where","Unpack","ZerosLike" #clear list
           ]
    fp32: ['*'] # '*' means all op types

  capabilities:
    int8: {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'FusedBatchNormV3': {
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'Conv3D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel', 'per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          '_MklFusedInstanceNorm': {
            'activation': {
                        'dtype': ['int8', 'fp32'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'BatchMatMul': {
            'weight': {
                        'dtype': ['int8', 'fp32'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8', 'fp32'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'BatchMatMulV2': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {

                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

    uint8: {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'Conv3D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel', 'per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'BatchMatMul': {
            'weight': {
                        'dtype': ['int8', 'fp32'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8', 'fp32'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'BatchMatMulV2': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

  patterns:
    fp32: [ #TODO Add more patterns here to demonstrate our concept the results external engine should return.
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd'
        ]
    int8: [
        'Dequantize + Conv2D + BiasAdd + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Add + Relu6 + Mul + Mul + QuantizeV2',
        'Dequantize + Conv2D + Add + Relu6 + Mul + Mul + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + swish_f32 + QuantizeV2',
        'Dequantize + Conv2D + Add + swish_f32 + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + swish_f32 + QuantizeV2',
        'Dequantize + Conv2D + swish_f32 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu + QuantizeV2',
        'Dequantize + Conv2D + Relu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Elu + QuantizeV2',
        'Dequantize + Conv2D + Elu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Add + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + AddV2 + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + Add + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Sigmoid + QuantizeV2',
        'Dequantize + Conv2D + Sigmoid + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + LeakyRelu + AddV2 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + LeakyRelu + Add + QuantizeV2',
        'Dequantize + Conv2D + LeakyRelu + AddV2 + QuantizeV2',
        'Dequantize + Conv2D + LeakyRelu + Add + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu + AddV2 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu + Add + QuantizeV2',
        'Dequantize + Conv2D + Relu + AddV2 + QuantizeV2',
        'Dequantize + Conv2D + Relu + Add + QuantizeV2',
        'Dequantize + Conv2D + Add + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + Add + QuantizeV2',
        'Dequantize + Conv2D + Add + Add + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Add + QuantizeV2',
        'Dequantize + Conv3D + Add + QuantizeV2',
        'Dequantize + Conv3D + AddV2 + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + Add + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + AddV2 + QuantizeV2',
        'Dequantize + Conv3D + AddV2 + AddV2 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + Add + Relu6 + Mul + Mul + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Add + Relu6 + Mul + Mul + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + swish_f32 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Add + swish_f32 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + AddV2 + swish_f32 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + swish_f32 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + LeakyRelu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + LeakyRelu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + Relu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Relu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Add + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + QuantizeV2',
        'Dequantize + FusedBatchNormV3 + Relu + QuantizeV2',
        'Dequantize + FusedBatchNormV3 + LeakyRelu + QuantizeV2',
        'Dequantize + _MklFusedInstanceNorm + Relu + QuantizeV2',
        'Dequantize + _MklFusedInstanceNorm + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2DBackpropInput + BiasAdd + QuantizeV2',
        'Dequantize + Conv3DBackpropInputV2 + BiasAdd + QuantizeV2'
        ]
    uint8: [
        'Dequantize + Conv2D + BiasAdd + AddN + Relu + QuantizeV2',
        'Dequantize + Conv2D + AddN + Relu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + AddN + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + AddN + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + AddV2 + Relu + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + Relu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + AddV2 + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + AddV2 + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Add + Relu + QuantizeV2',
        'Dequantize + Conv2D + Add + Relu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Add + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + Add + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + Add + Relu + QuantizeV2',
        'Dequantize + Conv2D + Add + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + Relu + QuantizeV2',
        'Dequantize + Conv2D + Relu6 + QuantizeV2',
        'Dequantize + Conv2D + BiasAdd + QuantizeV2',
        'Dequantize + Conv2D + Add + Add + Relu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + Relu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Relu + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + Add + Relu6 + QuantizeV2',
        'Dequantize + DepthwiseConv2dNative + BiasAdd + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Add + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + AddV2 + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Relu + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + LeakyRelu + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Gelu + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Elu + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Tanh + QuantizeV2',
        'Dequantize + MatMul + BiasAdd + Sigmoid + QuantizeV2',
        'Dequantize + MatMul + Add + QuantizeV2',
        'Dequantize + MatMul + AddV2 + QuantizeV2',
        'Dequantize + MatMul + Relu + QuantizeV2',
        'Dequantize + MatMul + Relu6 + QuantizeV2',
        'Dequantize + MatMul + LeakyRelu + QuantizeV2',
        'Dequantize + MatMul + Gelu + QuantizeV2',
        'Dequantize + MatMul + Elu + QuantizeV2',
        'Dequantize + MatMul + Tanh + QuantizeV2',
        'Dequantize + MatMul + Sigmoid + QuantizeV2',
        'Dequantize + BatchMatMul + Mul + QuantizeV2',
        'Dequantize + BatchMatMulV2 + Mul + QuantizeV2',
        'Dequantize + BatchMatMul + Add + QuantizeV2',
        'Dequantize + BatchMatMulV2 + Add + QuantizeV2',
        'Dequantize + BatchMatMul + AddV2 + QuantizeV2',
        'Dequantize + BatchMatMulV2 + AddV2 + QuantizeV2',
        'Dequantize + BatchMatMul + Mul + Add + QuantizeV2',
        'Dequantize + BatchMatMulV2 + Mul + Add + QuantizeV2',
        'Dequantize + BatchMatMul + Mul + AddV2 + QuantizeV2',
        'Dequantize + BatchMatMulV2 + Mul + AddV2 + QuantizeV2',
        'Dequantize + Conv3D + AddV2 + AddV2 + Relu + QuantizeV2',
        'Dequantize + Conv3D + Add + Relu + QuantizeV2',
        'Dequantize + Conv3D + AddV2 + Relu + QuantizeV2',
        'Dequantize + Conv3D + Relu + QuantizeV2',
        'Dequantize + Conv3D + Relu6 + QuantizeV2',
        'Dequantize + Conv3D + Add + Relu6 + QuantizeV2',
        'Dequantize + Conv3D + AddV2 + Relu6 + QuantizeV2',
        'Dequantize + Conv3D + Eelu + QuantizeV2',
        'Dequantize + Conv3D + LeakyRelu + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + Relu + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + Relu6 + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + Eelu + QuantizeV2',
        'Dequantize + Conv3D + BiasAdd + LeakyRelu + QuantizeV2',
        'Dequantize + Conv3D + Add + Relu + QuantizeV2',
        'Dequantize + Conv3D + Add + Relu6 + QuantizeV2',
        'Dequantize + Conv3D + Add + Eelu + QuantizeV2',
        'Dequantize + Conv3D + Add + LeakyRelu + QuantizeV2',
        'Dequantize + Conv2DBackpropInput + BiasAdd + QuantizeV2',
        'Dequantize + Conv3DBackpropInputV2 + BiasAdd + QuantizeV2'
  ]

  grappler_optimization:
    pruning: True                                    # optional. grappler pruning optimizer,default value is True.
    shape: True                                      # optional. grappler shape optimizer,default value is True.
    constfold: False                                 # optional. grappler constant folding optimizer, default value is True.
    arithmetic: False                                # optional. grappler arithmetic optimizer,default value is False.
    dependency: True                                 # optional. grappler dependency optimizer,default value is True.
    debug_stripper: True                             # optional. grappler debug_stripper optimizer,default value is True.
    loop: True                                       # optional. grappler loop optimizer,default value is True.


-
  version:
    name: ['2.1.0', '2.2.0', '2.3.0', '2.4.0', '2.5.0', '2.6.0', '2.6.1', '2.6.2', '2.7.0', '2.8.0', '2.9.0', '2.9.1', '2.10.0', '1.15.0-up1', '1.15.0-up2']

  precisions: &common_precisions
    names: int8, uint8, bf16, fp32
    valid_mixed_precisions: []

  ops: &common_ops
    int8: ['Conv2D', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
    uint8: ['Conv2D', 'DepthwiseConv2dNative', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
    bf16: ['Conv2D']  #TODO need to add more bf16 op types here
    fp32: ['*'] # '*' means all op types

  capabilities: &common_capabilities
    int8: &ref_2_4_int8 {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

    uint8: &ref_2_4_uint8 {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

  patterns: &common_patterns
    fp32: [ #TODO Add more patterns here to demonstrate our concept the results external engine should return.
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd'
        ]
    int8: [
        'Conv2D + BiasAdd',
        'Conv2D + BiasAdd + Relu',
        'Conv2D + BiasAdd + Relu6'
        ]
    uint8: [
        'Conv2D + BiasAdd + AddN + Relu',
        'Conv2D + BiasAdd + AddN + Relu6',
        'Conv2D + BiasAdd + AddV2 + Relu',
        'Conv2D + BiasAdd + AddV2 + Relu6',
        'Conv2D + BiasAdd + Add + Relu',
        'Conv2D + BiasAdd + Add + Relu6',
        'Conv2D + BiasAdd + Relu',
        'Conv2D + BiasAdd + Relu6',
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd',
        'DepthwiseConv2dNative + BiasAdd + Relu6',
        'DepthwiseConv2dNative + BiasAdd + Relu',
        'DepthwiseConv2dNative + Add + Relu6',
        'DepthwiseConv2dNative + BiasAdd',
        'MatMul + BiasAdd + Relu',
        'MatMul + BiasAdd',
  ]

  grappler_optimization: &common_grappler_optimization
    pruning: True                                    # optional. grappler pruning optimizer,default value is True.
    shape: True                                      # optional. grappler shape optimizer,default value is True.
    constfold: False                                 # optional. grappler constant folding optimizer, default value is True.
    arithmetic: False                                # optional. grappler arithmetic optimizer,default value is False.
    dependency: True                                 # optional. grappler dependency optimizer,default value is True.
    debug_stripper: True                             # optional. grappler debug_stripper optimizer,default value is True.
    loop: True                                       # optional. grappler loop optimizer,default value is True.

-
  version:
    name: '1.15.0-up3'

  precisions:
    names: int8, uint8, bf16, fp32
    valid_mixed_precisions: []

  ops:
    int8: ['Conv2D', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
    uint8: ['Conv2D', 'DepthwiseConv2dNative', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
    bf16: ['Conv2D']  #TODO need to add more bf16 op types here
    fp32: ['*'] # '*' means all op types

  capabilities:
    int8: {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['int8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

    uint8:  {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel','per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }

  patterns:
    fp32: [ #TODO Add more patterns here to demonstrate our concept the results external engine should return.
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd'
        ]
    int8: [
        'Conv2D + BiasAdd',
        'Conv2D + BiasAdd + Relu',
        'Conv2D + BiasAdd + LeakyRelu',
        'Conv2D + BiasAdd + LeakyRelu + AddV2',
        'Conv2D + BiasAdd + Relu6'
        ]
    uint8: [
        'Conv2D + BiasAdd + AddN + Relu',
        'Conv2D + BiasAdd + AddN + Relu6',
        'Conv2D + BiasAdd + AddV2 + Relu',
        'Conv2D + BiasAdd + AddV2 + Relu6',
        'Conv2D + BiasAdd + Add + Relu',
        'Conv2D + BiasAdd + Add + Relu6',
        'Conv2D + BiasAdd + Relu',
        'Conv2D + BiasAdd + Relu6',
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd',
        'DepthwiseConv2dNative + BiasAdd + Relu6',
        'DepthwiseConv2dNative + Add + Relu6',
        'DepthwiseConv2dNative + BiasAdd',
        'MatMul + BiasAdd + Relu',
        'MatMul + BiasAdd',
  ]

  grappler_optimization:
    pruning: True                                    # optional. grappler pruning optimizer,default value is True.
    shape: True                                      # optional. grappler shape optimizer,default value is True.
    constfold: False                                 # optional. grappler constant folding optimizer, default value is True.
    arithmetic: False                                # optional. grappler arithmetic optimizer,default value is False.
    dependency: True                                 # optional. grappler dependency optimizer,default value is True.
    debug_stripper: True                             # optional. grappler debug_stripper optimizer,default value is True.
    loop: True

-
  version:
    name: ['default', '1.15.0', '1.15.2', '2.0.0', '2.0.1']

  precisions: &default_precisions
    names: uint8, fp32
    valid_mixed_precisions: []

  ops: &default_ops
    int8: ['MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
    uint8: ['Conv2D', 'DepthwiseConv2dNative','MatMul', 'ConcatV2','MaxPool', 'AvgPool']
    fp32: ['*']

  capabilities: &default_capabilities
    uint8:  {
          'Conv2D': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_channel', 'per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax', 'kl']
                        }
                    },
          'MatMul': {
            'weight': {
                        'dtype': ['int8'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        },
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'scheme': ['asym', 'sym'],
                        'granularity': ['per_tensor'],
                        'algorithm': ['minmax']
                        }
                    },
          'default': {
            'activation': {
                        'dtype': ['uint8'],
                        'quant_mode': 'static',
                        'algorithm': ['minmax'],
                        'scheme': ['sym'],
                        'granularity': ['per_tensor']
                        }
                    },
          }
    int8: {}

  patterns: &default_patterns
    fp32: [ #TODO Add more patterns here to demonstrate our concept the results external engine should return.
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd',
        ]
    int8: [
        'MatMul + BiasAdd + Relu',
        'MatMul + BiasAdd'
        ]
    uint8: [
        'Conv2D + BiasAdd + AddN + Relu',
        'Conv2D + BiasAdd + AddN + Relu6',
        'Conv2D + BiasAdd + AddV2 + Relu',
        'Conv2D + BiasAdd + AddV2 + Relu6',
        'Conv2D + BiasAdd + Add + Relu',
        'Conv2D + BiasAdd + Add + Relu6',
        'Conv2D + BiasAdd + Relu',
        'Conv2D + BiasAdd + Relu6',
        'Conv2D + Add + Relu',
        'Conv2D + Add + Relu6',
        'Conv2D + Relu',
        'Conv2D + Relu6',
        'Conv2D + BiasAdd',
        'DepthwiseConv2dNative + BiasAdd + Relu6',
        'DepthwiseConv2dNative + Add + Relu6',
        'DepthwiseConv2dNative + BiasAdd',
        'MatMul + BiasAdd + Relu',
        'MatMul + BiasAdd',
  ]

  grappler_optimization: &default_grappler_optimization
    pruning: True                                   # optional. grappler pruning optimizer,default value is False.
    shape: True                                      # optional. grappler shape optimizer,default value is True.
    dependency: True                                 # optional. grappler dependency optimizer,default value is True.
    debug_stripper: True                             # optional. grappler debug_stripper optimizer,default value is True.
    loop: True                                       # optional. grappler loop optimizer,default value is True.
