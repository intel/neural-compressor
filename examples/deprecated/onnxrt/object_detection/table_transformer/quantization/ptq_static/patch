diff --git a/detr/models/backbone.py b/detr/models/backbone.py
index b16f1b0..e99ba52 100644
--- a/detr/models/backbone.py
+++ b/detr/models/backbone.py
@@ -11,7 +11,7 @@ from torch import nn
 from torchvision.models._utils import IntermediateLayerGetter
 from typing import Dict, List
 
-from util.misc import NestedTensor, is_main_process
+from util.misc import NestedTensor, is_main_process, nested_tensor_from_tensor_list
 
 from .position_encoding import build_position_encoding
 
@@ -69,15 +69,14 @@ class BackboneBase(nn.Module):
         self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)
         self.num_channels = num_channels
 
-    def forward(self, tensor_list: NestedTensor):
-        xs = self.body(tensor_list.tensors)
-        out: Dict[str, NestedTensor] = {}
+    def forward(self, image, m):
+        xs = self.body(image)
+        out = []
+        mask = []
         for name, x in xs.items():
-            m = tensor_list.mask
-            assert m is not None
-            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]
-            out[name] = NestedTensor(x, mask)
-        return out
+            out.append(x)
+            mask.append(F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0])
+        return out, mask
 
 
 class Backbone(BackboneBase):
@@ -97,16 +96,15 @@ class Joiner(nn.Sequential):
     def __init__(self, backbone, position_embedding):
         super().__init__(backbone, position_embedding)
 
-    def forward(self, tensor_list: NestedTensor):
-        xs = self[0](tensor_list)
-        out: List[NestedTensor] = []
+    def forward(self, x, mask):
+        xs, ms = self[0](x, mask)
+        out = []
         pos = []
-        for name, x in xs.items():
-            out.append(x)
+        for x, m in zip(xs, ms):
             # position encoding
-            pos.append(self[1](x).to(x.tensors.dtype))
+            pos.append(self[1](x, m).to(x.dtype))
 
-        return out, pos
+        return (xs, ms), pos
 
 
 def build_backbone(args):
diff --git a/detr/models/detr.py b/detr/models/detr.py
index 19069f1..e8cbf84 100644
--- a/detr/models/detr.py
+++ b/detr/models/detr.py
@@ -41,7 +41,7 @@ class DETR(nn.Module):
         self.backbone = backbone
         self.aux_loss = aux_loss
 
-    def forward(self, samples: NestedTensor):
+    def forward(self, image, mask):
         """Â The forward expects a NestedTensor, which consists of:
                - samples.tensor: batched images, of shape [batch_size x 3 x H x W]
                - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels
@@ -56,11 +56,9 @@ class DETR(nn.Module):
                - "aux_outputs": Optional, only returned when auxilary losses are activated. It is a list of
                                 dictionnaries containing the two above keys for each decoder layer.
         """
-        if isinstance(samples, (list, torch.Tensor)):
-            samples = nested_tensor_from_tensor_list(samples)
-        features, pos = self.backbone(samples)
+        features, pos = self.backbone(image, mask)
+        src, mask = features[0][-1], features[1][-1]
 
-        src, mask = features[-1].decompose()
         assert mask is not None
         hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]
 
diff --git a/detr/models/position_encoding.py b/detr/models/position_encoding.py
index 73ae39e..2049449 100644
--- a/detr/models/position_encoding.py
+++ b/detr/models/position_encoding.py
@@ -25,9 +25,7 @@ class PositionEmbeddingSine(nn.Module):
             scale = 2 * math.pi
         self.scale = scale
 
-    def forward(self, tensor_list: NestedTensor):
-        x = tensor_list.tensors
-        mask = tensor_list.mask
+    def forward(self, x, mask):
         assert mask is not None
         not_mask = ~mask
         y_embed = not_mask.cumsum(1, dtype=torch.float32)
@@ -62,8 +60,7 @@ class PositionEmbeddingLearned(nn.Module):
         nn.init.uniform_(self.row_embed.weight)
         nn.init.uniform_(self.col_embed.weight)
 
-    def forward(self, tensor_list: NestedTensor):
-        x = tensor_list.tensors
+    def forward(self, x, mask):
         h, w = x.shape[-2:]
         i = torch.arange(w, device=x.device)
         j = torch.arange(h, device=x.device)
diff --git a/src/eval.py b/src/eval.py
index e3a0565..d66b318 100644
--- a/src/eval.py
+++ b/src/eval.py
@@ -4,6 +4,7 @@ Copyright (C) 2021 Microsoft Corporation
 import os
 import sys
 from collections import Counter
+import onnxruntime as ort
 import json
 import statistics as stat
 from datetime import datetime
@@ -575,7 +576,6 @@ def visualize(args, target, pred_logits, pred_bboxes):
 @torch.no_grad()
 def evaluate(args, model, criterion, postprocessors, data_loader, base_ds, device):
     st_time = datetime.now()
-    model.eval()
     criterion.eval()
 
     metric_logger = utils.MetricLogger(delimiter="  ")
@@ -594,6 +594,8 @@ def evaluate(args, model, criterion, postprocessors, data_loader, base_ds, devic
     num_batches = len(data_loader)
     print_every = max(args.eval_step, int(math.ceil(num_batches / 100)))
     batch_num = 0
+    session = ort.InferenceSession(model.SerializeToString(), providers=['CPUExecutionProvider']) if \
+        not isinstance(model, str) else ort.InferenceSession(model, providers=['CPUExecutionProvider'])
 
     for samples, targets in metric_logger.log_every(data_loader, print_every, header):
         batch_num += 1
@@ -603,7 +605,12 @@ def evaluate(args, model, criterion, postprocessors, data_loader, base_ds, devic
                 if not k == 'img_path':
                     t[k] = v.to(device)
 
-        outputs = model(samples)
+        ort_input = {'image': samples.tensors.detach().numpy(),
+            'mask': samples.mask.detach().numpy()}
+        output = session.run(None, ort_input)
+        outputs_class = torch.from_numpy(output[0])
+        outputs_coord = torch.from_numpy(output[1])
+        outputs = {'pred_logits': torch.from_numpy(output[0]), 'pred_boxes': torch.from_numpy(output[1])}
 
         if args.debug:
             for target, pred_logits, pred_boxes in zip(targets, outputs['pred_logits'], outputs['pred_boxes']):
@@ -696,3 +703,4 @@ def eval_coco(args, model, criterion, postprocessors, data_loader_test, dataset_
     print("COCO metrics summary: AP50: {:.3f}, AP75: {:.3f}, AP: {:.3f}, AR: {:.3f}".format(
         pubmed_stats['coco_eval_bbox'][1], pubmed_stats['coco_eval_bbox'][2],
         pubmed_stats['coco_eval_bbox'][0], pubmed_stats['coco_eval_bbox'][8]))
+    return pubmed_stats['coco_eval_bbox'][0]
\ No newline at end of file
diff --git a/src/main.py b/src/main.py
index 74cd13c..c30377d 100644
--- a/src/main.py
+++ b/src/main.py
@@ -41,6 +41,7 @@ def get_args():
         default='structure',
         help="toggle between structure recognition and table detection")
     parser.add_argument('--model_load_path', help="The path to trained model")
+    parser.add_argument('--input_onnx_model', type=str, help="The path to onnx model")
     parser.add_argument('--load_weights_only', action='store_true')
     parser.add_argument('--model_save_dir', help="The output directory for saving model params and checkpoints")
     parser.add_argument('--metrics_save_filepath',
@@ -52,9 +53,9 @@ def get_args():
     parser.add_argument('--table_words_dir',
                         help="Folder containg the bboxes of table words")
     parser.add_argument('--mode',
-                        choices=['train', 'eval'],
+                        choices=['train', 'accuracy', 'export', 'quantize', 'performance'],
                         default='train',
-                        help="Modes: training (train) and evaluation (eval)")
+                        help="Modes: training (train) and evaluation (eval) and export model to onnx format")
     parser.add_argument('--debug', action='store_true')
     parser.add_argument('--device')
     parser.add_argument('--lr', type=float)
@@ -69,6 +70,8 @@ def get_args():
     parser.add_argument('--test_max_size', type=int)
     parser.add_argument('--eval_pool_size', type=int, default=1)
     parser.add_argument('--eval_step', type=int, default=1)
+    parser.add_argument('--output_model', type=str)
+    parser.add_argument('--opset_version', type=int, default=13)
 
     return parser.parse_args()
 
@@ -95,6 +98,14 @@ def get_class_map(data_type):
         class_map = {'table': 0, 'table rotated': 1, 'no object': 2}
     return class_map
 
+class OXDataloader:
+    def __init__(self, pt_dataloader, batch_size):
+        self.dl = pt_dataloader
+        self.batch_size = batch_size
+
+    def __iter__(self):
+        for samples, targets in self.dl:
+            yield (samples.tensors.detach().numpy(), samples.mask.detach().numpy()), targets
 
 def get_data(args):
     """
@@ -147,7 +158,7 @@ def get_data(args):
         return data_loader_train, data_loader_val, dataset_val, len(
             dataset_train)
 
-    elif args.mode == "eval":
+    elif args.mode in ["accuracy", "export"]:
 
         dataset_test = PDFTablesDataset(os.path.join(args.data_root_dir,
                                                      "test"),
@@ -169,6 +180,28 @@ def get_data(args):
                                       num_workers=args.num_workers)
         return data_loader_test, dataset_test
 
+    elif args.mode in ["quantize", "performance"]:
+
+        dataset_test = PDFTablesDataset(os.path.join(args.data_root_dir,
+                                                     "test"),
+                                        get_transform(args.data_type, "val"),
+                                        do_crop=False,
+                                        max_size=args.test_max_size,
+                                        make_coco=True,
+                                        include_eval=True,
+                                        image_extension=".jpg",
+                                        xml_fileset="test_filelist.txt",
+                                        class_map=class_map)
+        sampler_test = torch.utils.data.SequentialSampler(dataset_test)
+
+        data_loader_test = DataLoader(dataset_test,
+                                      2 * args.batch_size,
+                                      sampler=sampler_test,
+                                      drop_last=False,
+                                      collate_fn=utils.collate_fn,
+                                      num_workers=args.num_workers)
+        return OXDataloader(data_loader_test, args.batch_size), dataset_test
+
     elif args.mode == "grits" or args.mode == "grits-all":
         dataset_test = PDFTablesDataset(os.path.join(args.data_root_dir,
                                                      "test"),
@@ -337,6 +370,20 @@ def train(args, model, criterion, postprocessors, device):
 
     print('Total training time: ', datetime.now() - start_time)
 
+def export(args, model, data_loader, device):
+    model.eval()
+    for samples, targets in data_loader:
+        samples = samples.to(device)
+        torch.onnx.export(model,
+                          (samples.tensors, samples.mask),
+                          args.output_model,
+                          opset_version=args.opset_version,
+                          do_constant_folding=True,
+                          input_names=["image", "mask"],
+                          dynamic_axes={"image":{0: "batch_size", 1: "channel", 2: "height", 3: "width"},
+                                        "mask":{0: "batch_size", 1: "height", 2: "width"}})
+        print('Export onnx model to {}.'.format(args.output_model))
+        break
 
 def main():
     cmd_args = get_args().__dict__
@@ -350,7 +397,7 @@ def main():
     print('-' * 100)
 
     # Check for debug mode
-    if args.mode == 'eval' and args.debug:
+    if args.mode == 'accuracy' and args.debug:
         print("Running evaluation/inference in DEBUG mode, processing will take longer. Saving output to: {}.".format(args.debug_save_dir))
         os.makedirs(args.debug_save_dir, exist_ok=True)
 
@@ -366,10 +413,35 @@ def main():
 
     if args.mode == "train":
         train(args, model, criterion, postprocessors, device)
-    elif args.mode == "eval":
+    elif args.mode == "accuracy":
         data_loader_test, dataset_test = get_data(args)
-        eval_coco(args, model, criterion, postprocessors, data_loader_test, dataset_test, device)
-
+        ap_result = eval_coco(args, args.input_onnx_model, criterion, postprocessors, data_loader_test, dataset_test, device)
+        print("Batch size = %d" % args.batch_size)
+        print("Accuracy: %.5f" % ap_result)
+    elif args.mode == "export":
+        data_loader_test, dataset_test = get_data(args)
+        export(args, model, data_loader_test, device)
+    elif args.mode == "quantize":
+        data_loader_test, dataset_test = get_data(args)
+        from neural_compressor import quantization, PostTrainingQuantConfig
+        config = PostTrainingQuantConfig(
+            recipes={'optypes_to_exclude_output_quant': ['MatMul'],
+                     'gemm_to_matmul': False
+            },
+            op_name_dict={
+                     '/transformer/decoder.*': {"activation": {"dtype": ["fp32"]}, "weight": {"dtype": ["fp32"]}},
+                     '/bbox_embed.*': {"activation": {"dtype": ["fp32"]}, "weight": {"dtype": ["fp32"]}},
+                     '/Sigmoid': {"activation": {"dtype": ["fp32"]}, "weight": {"dtype": ["fp32"]}},
+                     '/class_embed/MatMul': {"activation": {"dtype": ["fp32"]}, "weight": {"dtype": ["fp32"]}},
+            })
+        q_model = quantization.fit(args.input_onnx_model, config, calib_dataloader=data_loader_test)
+        q_model.save(args.output_model)
+    elif args.mode == "performance":
+        data_loader_test, dataset_test = get_data(args)
+        from neural_compressor.benchmark import fit
+        from neural_compressor.config import BenchmarkConfig
+        config = BenchmarkConfig(warmup=10, iteration=100, cores_per_instance=4, num_of_instance=1)
+        fit(args.input_onnx_model, config, b_dataloader=data_loader_test)
 
 if __name__ == "__main__":
     main()
