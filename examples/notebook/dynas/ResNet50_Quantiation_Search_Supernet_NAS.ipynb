{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial demonstrates how to perform a multi-objective neural architecture search (NAS) and Quantization Policy (QP) search on a ResNet50 one-shot weight-sharing super-network [1] using the Intel® Neural Compressor Dynamic NAS (DyNAS) search approach. \n",
    "\n",
    "#### Background\n",
    "Neural architecture search, the study of automating the discovery of optimal deep neural network architectures for tasks in domains such as computer vision and natural language processing, has seen rapid growth in the machine learning research community. While there have been many recent advancements in NAS, there is still a significant focus on reducing the computational cost incurred when validating discovered architectures by making search more efficient. Evolutionary algorithms, specifically genetic algorithms, have a history of usage in NAS and continue to gain popularity as a highly efficient way to explore the architecture objective space. In this tutorial, we show how evolutionary algorithms [2] can be paired with lightly trained objective predictors in an iterative cycle to accelerate multi-objective architectural exploration. Specifically, we use a bi-level optimization approach [3] denoted as `dynas`. This technique is ~4x more sample efficient than typical one-shot predictor-based NAS approaches. \n",
    "\n",
    "#### Super-Networks\n",
    "\n",
    "The computational overhead of evaluating DNN architectures during the neural architecture search process can be very costly due to the training and validation cycles. To address the training overhead, novel weight-sharing approaches known as one-shot or super-networks have offered a way to mitigate the training overhead by reducing training times from thousands to a few GPU days. These approaches train a task-specific super-network architecture with a weight-sharing mechanism that allows the sub-networks to be treated as unique individual architectures. This enables sub-network model extraction and validation without a separate training cycle. This tutorial offers pre-trained Once-for-All (OFA) super-networks [1] for the image classification on ImageNet-ilsvrc2012 as well as Transformer Language Translation (based on [6]) for the language translation tasks.\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "The flow of the DyNAS approach (`approach='dynas'`) is shown in the following figure. Currently, three pre-trained super-network options for the image classification task are provided. In the first phase of the search, a small population (`config.dynas.population`) of sub-networks are randomly sampled and evaluated (validation measurement) to provide the initial training set for the inner predictor loop. After the predictors are trained, a multi-objective evolutionary search (`search_algorithm`) is performed in the predictor objective space. After an extensive search is performed, the best performing sub-network configurations are selected to be the next iteration's validation population. The cycle continues until the search concludes when the user defined evaluation count (`config.dynas.num_evals`) is met. \n",
    "   \n",
    "<br>\n",
    "<div>\n",
    "<img src=\"DyNAS_flow.png\" width=\"750\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install neural_compressor dynast==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatievely, if you have a local copy of https://github.com/intel/neural-compressor, you can uncomment and run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0,'<path to neural compressor>')\n",
    "# !pip install -qr <path to neural compressor>/requirements.txt\n",
    "# !pip install -q dynast==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_compressor.conf.config import NASConfig\n",
    "from neural_compressor.experimental.nas import NAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure NAS Algorithm\n",
    "\n",
    "The `NASConfig` class allows us to define the appropriate paramenters for determining how the neural architecture search is performed. Currently, the following multi-objective evolutionary algorithms are supported by the `dynas` approach: \n",
    "* `'nsga2'`\n",
    "* `'age'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NASConfig(approach='dynas', search_algorithm='nsga2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture\n",
    "We currently support pre-trained super-networks:\n",
    "\n",
    "1. Once-for-All (OFA) super-networks [4] for the image classification task on ImageNet-ilsvrc2012. In the case where the super-network PyTorch model download fails, you can manually copy the pre-trained models from https://github.com/mit-han-lab/once-for-all and place them in the `.torch/ofa_nets` path.\n",
    "2. Hardware-Aware-Transformers (HAT) supernetwork [6] for language translation task on WMT14 En-De. To run this supernetwork you have to manually download preprocessed dataset from https://github.com/mit-han-lab/hardware-aware-transformers/blob/master/configs/wmt14.en-de/get_preprocessed.sh and pretrained model from https://www.dropbox.com/s/pkdddxvvpw9a4vq/HAT_wmt14ende_super_space0.pt?dl=0\n",
    "3. BERT Base supernetwork finetuned on the [Stanford Sentiment Treebank](https://huggingface.co/datasets/sst2) dataset. You can download the model from [here](https://github.com/IntelLabs/DyNAS-T/blob/bert/model/models/fine_tuned_bert_sst2.pt) and the script to prepare the SST-2 dataset is available [here](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/data/create_datasets_from_start.sh#L21).\n",
    "\n",
    "Super-network options (choose 1): \n",
    "- `ofa_resnet50` - based on the ResNet50 architecture [4]. Search space of ~$10^{15}$ architectures.\n",
    "- `inc_quantization_ofa_resnet50` - based on the ResNet50 architecture [4]. This search is performed on `ofa_resnet50` FP32 super-network and is extended with a Quantization Policy search.\n",
    "- `ofa_mbv3_d234_e346_k357_w1.0` - based on the MobileNetV3 architecture [5], width multiplier 1.0. Search space of ~$10^{19}$ architectures.\n",
    "- `ofa_mbv3_d234_e346_k357_w1.2` - based on the MobileNetV3 architecture [5], width multiplier 1.2. Search space of ~$10^{19}$ architectures.  \n",
    "- `transformer_lt_wmt_en_de` - based on the Transformer architecture [7].\n",
    "- `bert_base_sst2` - based on the BERT Base architecture, fine-tuned on the [Stanford Sentiment Treebank](https://huggingface.co/datasets/sst2) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dynas.supernet = 'inc_quantization_ofa_resnet50'\n",
    "config.nas.search.seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select performance metrics\n",
    "\n",
    "Performance metric options are as follows. You can use from 1 to 3 performance objectives to guide the search.\n",
    "\n",
    "Description:\n",
    "* `'accuracy_top1' or 'bleu' or 'accuracy_sst2'` - ImageNet Top-1 Accuracy (%) (for OFA supetnetworks) and Bleu (for Transformer LT) and accuracy on the SST2 dataset.\n",
    "* `'macs'` - Multiply-and-accumulates as measured from FVCore. \n",
    "* `'latency'` - Latency (inference time) measurement (ms).\n",
    "* `'params'` - Number of parameters in the model.\n",
    "* `'model_size'` - Size of the model in MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dynas.metrics = ['accuracy_top1', 'latency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search parameters\n",
    "\n",
    "* `config.dynas.population` - Size of the population for evolutionary/genetic algorithm (50 recommended)\n",
    "* `config.dynas.num_evals` - Validation measurement count, a higher count comes with greater computational cost but a higher chance of finding optimal sub-networks\n",
    "* `config.dynas.results_csv_path` - Location of the search (validation measurement) results. This file is also used to provide training data to the metric predictors. \n",
    "* `config.dynas.batch_size` - Batch size used during latency measurements.\n",
    "* `config.dynas.eval_batch_size` - Batch size used during accuracy measurements.\n",
    "* `config.dynas.dataset_path` - For OFA it's a path to the imagenet-ilsvrc2012 dataset. This can be obtained at: https://www.image-net.org/download.php; For Transformer LT it's a path to preprocessed WMT EnDe directory (`(...)/data/binary/wmt16_en_de`)\n",
    "* `config.dynas.supernet_ckpt_path` - Transformer LT only. Path to downloaded pretrained super-network (`HAT_wmt14ende_super_space0.pt` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dynas.population = 50\n",
    "config.dynas.num_evals = 250\n",
    "config.dynas.results_csv_path = 'search_results.csv'\n",
    "config.dynas.batch_size = 1\n",
    "config.dynas.eval_batch_size = 64\n",
    "config.dynas.dataset_path = '/localdisk/dataset/imagenet' #example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Search\n",
    "\n",
    "After the DyNAS configuration parameters are set, the search process can be started. Depending on how many evaluations `config.dynas.num_evals` were defined, the search time can vary from hours to days. \n",
    "The search process will populate the `config.dynas.results_csv_path` file and will also return a list of the final iteration's best sub-network population recommondation. \n",
    "\n",
    "Note: example search results are provided for the plotting section if you wish to skip this step for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 14:08:28 [INFO] init_cfg\n",
      "/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pymoo/algorithms/moo/age.py:212: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(fastmath=True)\n",
      "/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pymoo/algorithms/moo/age.py:222: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(fastmath=True)\n",
      "/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pymoo/algorithms/moo/age.py:257: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(fastmath=True)\n",
      "[09-05 14:08:29 MainProcess #3010415] INFO  text_to_speech.py:34 - Please install tensorboardX: pip install tensorboardX\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:69 - ========================================\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:70 - Starting Dynamic NAS Toolkit (DyNAS-T)\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:71 - ========================================\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:79 - ----------------------------------------\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:80 - DyNAS Parameter Inputs:\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - seed: 42\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - population: 50\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - batch_size: 1\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - eval_batch_size: 64\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - search_algo: nsga2\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - supernet_ckpt_path: None\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - dataloader_workers: 20\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - distributed: False\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - test_fraction: 1.0\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - supernet: inc_quantization_ofa_resnet50\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - results_path: search_results.csv\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - optimization_metrics: ['accuracy_top1', 'latency']\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - measurements: ['accuracy_top1', 'latency']\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - search_tactic: linas\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - num_evals: 250\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:82 - dataset_path: /localdisk/dataset/imagenet\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:83 - ----------------------------------------\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  dynast_manager.py:98 - Initializing DyNAS LINAS algorithm object.\n",
      "[09-05 14:08:30 MainProcess #3010415] ERROR search_tactic.py:143 - Invalid supernet specified. Choose from the following: {'image_classification': ['ofa_resnet50', 'ofa_mbv3_d234_e346_k357_w1.0', 'ofa_mbv3_d234_e346_k357_w1.2', 'ofa_proxyless_d234_e346_k357_w1.3', 'bootstrapnas_image_classification'], 'machine_translation': ['transformer_lt_wmt_en_de'], 'text_classification': ['bert_base_sst2'], 'quantization': ['inc_quantization_ofa_resnet50'], 'recommendation': []}\n",
      "[09-05 14:08:30 MainProcess #3010415] INFO  search_tactic.py:148 - Results csv file header ordering will be: ['Sub-network', 'Date', 'Model Parameters', 'Latency (ms)', 'Model Size', 'Top-1 Acc (%)']\n",
      "Downloading: \"https://hanlab.mit.edu/files/OnceForAll/ofa_nets/ofa_resnet50_d=0+1+2_e=0.2+0.25+0.35_w=0.65+0.8+1.0\" to .torch/ofa_nets/ofa_resnet50_d=0+1+2_e=0.2+0.25+0.35_w=0.65+0.8+1.0\n",
      "[09-05 14:08:46 MainProcess #3010415] INFO  evaluation_interface.py:49 - (Re)Formatted results file: search_results.csv\n",
      "[09-05 14:08:46 MainProcess #3010415] INFO  evaluation_interface.py:50 - csv file header: ['Sub-network', 'Date', 'Model Parameters', 'Latency (ms)', 'Model Size', 'Top-1 Acc (%)']\n",
      "[09-05 14:08:46 MainProcess #3010415] INFO  search_tactic.py:354 - Starting LINAS loop 1 of 5.\n",
      "[09-05 14:08:46 MainProcess #3010415] INFO  search_tactic.py:358 - Evaluating subnetwork 1/50 in loop 1 of 5\n",
      "2023-09-05 14:08:47 [WARNING] Force convert framework model to neural_compressor model.\n",
      "2023-09-05 14:08:47 [INFO] Because both eval_dataloader_cfg and user-defined eval_func are None, automatically setting 'tuning.exit_policy.performance_only = True'.\n",
      "2023-09-05 14:08:47 [INFO] The cfg.tuning.exit_policy.performance_only is: True\n",
      "2023-09-05 14:08:47 [INFO] Attention Blocks: 0\n",
      "2023-09-05 14:08:47 [INFO] FFN Blocks: 0\n",
      "2023-09-05 14:08:47 [INFO] Pass query framework capability elapsed time: 453.56 ms\n",
      "2023-09-05 14:08:47 [INFO] Adaptor has 2 recipes.\n",
      "2023-09-05 14:08:47 [INFO] 0 recipes specified by user.\n",
      "2023-09-05 14:08:47 [INFO] 0 recipes require future tuning.\n",
      "2023-09-05 14:08:47 [INFO] Neither evaluation function nor metric is defined. Generate a quantized model with default quantization configuration.\n",
      "2023-09-05 14:08:47 [INFO] Force setting 'tuning.exit_policy.performance_only = True'.\n",
      "2023-09-05 14:08:48 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 64. So the real sampling size is 128.\n",
      "2023-09-05 14:08:58 [INFO] |*********Mixed Precision Statistics********|\n",
      "2023-09-05 14:08:58 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:08:58 [INFO] |       Op Type       | Total | INT8 | FP32 |\n",
      "2023-09-05 14:08:58 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:08:58 [INFO] |      ConvReLU2d     |   29  |  10  |  19  |\n",
      "2023-09-05 14:08:58 [INFO] | quantize_per_tensor |   17  |  17  |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |         add         |   1   |  1   |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |      dequantize     |   15  |  15  |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |      MaxPool2d      |   1   |  1   |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |        Conv2d       |   17  |  10  |  7   |\n",
      "2023-09-05 14:08:58 [INFO] |       add_relu      |   13  |  13  |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |      AvgPool2d      |   1   |  1   |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |         mean        |   2   |  2   |  0   |\n",
      "2023-09-05 14:08:58 [INFO] |        Linear       |   1   |  0   |  1   |\n",
      "2023-09-05 14:08:58 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:08:58 [INFO] Pass quantize model elapsed time: 10570.18 ms\n",
      "2023-09-05 14:08:58 [INFO] Save tuning history to /localdisk/maciej/code/neural-compressor/examples/notebook/dynas/nc_workspace/2023-09-05_14-07-22/./history.snapshot.\n",
      "2023-09-05 14:08:58 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2023-09-05 14:08:58 [INFO] Save deploy yaml to /localdisk/maciej/code/neural-compressor/examples/notebook/dynas/nc_workspace/2023-09-05_14-07-22/deploy.yaml\n",
      "2023-09-05 14:10:22 [WARNING] Force convert framework model to neural_compressor model.\n",
      "2023-09-05 14:10:22 [INFO] Because both eval_dataloader_cfg and user-defined eval_func are None, automatically setting 'tuning.exit_policy.performance_only = True'.\n",
      "2023-09-05 14:10:22 [INFO] The cfg.tuning.exit_policy.performance_only is: True\n",
      "2023-09-05 14:10:22 [INFO] Attention Blocks: 0\n",
      "2023-09-05 14:10:22 [INFO] FFN Blocks: 0\n",
      "2023-09-05 14:10:22 [INFO] Pass query framework capability elapsed time: 197.49 ms\n",
      "2023-09-05 14:10:22 [INFO] Adaptor has 2 recipes.\n",
      "2023-09-05 14:10:22 [INFO] 0 recipes specified by user.\n",
      "2023-09-05 14:10:22 [INFO] 0 recipes require future tuning.\n",
      "2023-09-05 14:10:22 [INFO] Neither evaluation function nor metric is defined. Generate a quantized model with default quantization configuration.\n",
      "2023-09-05 14:10:22 [INFO] Force setting 'tuning.exit_policy.performance_only = True'.\n",
      "2023-09-05 14:10:23 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 64. So the real sampling size is 128.\n",
      "2023-09-05 14:10:32 [INFO] |*********Mixed Precision Statistics********|\n",
      "2023-09-05 14:10:32 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:10:32 [INFO] |       Op Type       | Total | INT8 | FP32 |\n",
      "2023-09-05 14:10:32 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:10:32 [INFO] |      ConvReLU2d     |   29  |  10  |  19  |\n",
      "2023-09-05 14:10:32 [INFO] | quantize_per_tensor |   17  |  17  |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |         add         |   1   |  1   |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |      dequantize     |   15  |  15  |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |      MaxPool2d      |   1   |  1   |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |        Conv2d       |   17  |  10  |  7   |\n",
      "2023-09-05 14:10:32 [INFO] |       add_relu      |   13  |  13  |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |      AvgPool2d      |   1   |  1   |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |         mean        |   2   |  2   |  0   |\n",
      "2023-09-05 14:10:32 [INFO] |        Linear       |   1   |  0   |  1   |\n",
      "2023-09-05 14:10:32 [INFO] +---------------------+-------+------+------+\n",
      "2023-09-05 14:10:32 [INFO] Pass quantize model elapsed time: 9604.32 ms\n",
      "2023-09-05 14:10:32 [INFO] Save tuning history to /localdisk/maciej/code/neural-compressor/examples/notebook/dynas/nc_workspace/2023-09-05_14-07-22/./history.snapshot.\n",
      "2023-09-05 14:10:32 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2023-09-05 14:10:32 [INFO] Save deploy yaml to /localdisk/maciej/code/neural-compressor/examples/notebook/dynas/nc_workspace/2023-09-05_14-07-22/deploy.yaml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[39m=\u001b[39m NAS(config)\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msearch()\n",
      "File \u001b[0;32m/localdisk/maciej/code/neural-compressor/neural_compressor/experimental/nas/dynas.py:71\u001b[0m, in \u001b[0;36mDyNAS.search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the search process.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m        Best model architectures found in the search process.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynas_manager\u001b[39m.\u001b[39;49msearch()\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/dynast/search/search_tactic.py:359\u001b[0m, in \u001b[0;36mLINAS.search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mfor\u001b[39;00m _, individual \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(latest_population):\n\u001b[1;32m    358\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEvaluating subnetwork \u001b[39m\u001b[39m{\u001b[39;00m_\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation\u001b[39m}\u001b[39;00m\u001b[39m in loop \u001b[39m\u001b[39m{\u001b[39;00mloop\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mnum_loops\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidation_interface\u001b[39m.\u001b[39;49meval_subnet(individual)\n\u001b[1;32m    361\u001b[0m \u001b[39m# Inner-loop Low-Fidelity Predictor Runner, need to re-instantiate every loop\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_predictors()\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/dynast/supernetwork/image_classification/ofa_quantization/quantization_interface.py:363\u001b[0m, in \u001b[0;36mEvaluationInterfaceQuantizedOFAResNet50.eval_subnet\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    357\u001b[0m         individual_results[\u001b[39m'\u001b[39m\u001b[39mlatency\u001b[39m\u001b[39m'\u001b[39m], _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mmeasure_latency(\n\u001b[1;32m    358\u001b[0m             subnet_sample,\n\u001b[1;32m    359\u001b[0m             measure_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m    360\u001b[0m         )  \u001b[39m# For quantization latency stability it's better to measure over more steps\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maccuracy_top1\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeasurements:\n\u001b[0;32m--> 363\u001b[0m         individual_results[\u001b[39m'\u001b[39m\u001b[39maccuracy_top1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluator\u001b[39m.\u001b[39;49mvalidate_quantized_top1(subnet_sample)\n\u001b[1;32m    365\u001b[0m \u001b[39m# Write result for csv_path\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv_path:\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/dynast/supernetwork/image_classification/ofa_quantization/quantization_interface.py:200\u001b[0m, in \u001b[0;36mQuantizedOFARunner.validate_quantized_top1\u001b[0;34m(self, subnet_cfg, device)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m# Test sampled subnet\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_config\u001b[39m.\u001b[39mdata_provider\u001b[39m.\u001b[39massign_active_img_size(subnet_cfg[\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 200\u001b[0m loss, top1, top5 \u001b[39m=\u001b[39m validate_classification(\n\u001b[1;32m    201\u001b[0m     model\u001b[39m=\u001b[39;49msubnet_qt,\n\u001b[1;32m    202\u001b[0m     data_loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader,\n\u001b[1;32m    203\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m top1\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/dynast/utils/__init__.py:78\u001b[0m, in \u001b[0;36mmeasure_time.<locals>.wrapper_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m> Calling \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[1;32m     77\u001b[0m start_time \u001b[39m=\u001b[39m _time\u001b[39m.\u001b[39mperf_counter()  \u001b[39m# 1\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m value \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     79\u001b[0m end_time \u001b[39m=\u001b[39m _time\u001b[39m.\u001b[39mperf_counter()  \u001b[39m# 2\u001b[39;00m\n\u001b[1;32m     80\u001b[0m run_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time  \u001b[39m# 3\u001b[39;00m\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/dynast/utils/nn.py:100\u001b[0m, in \u001b[0;36mvalidate_classification\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mValidate #\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     88\u001b[0m         batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 100\u001b[0m output \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m    102\u001b[0m loss \u001b[39m=\u001b[39m test_criterion(output, labels)\n\u001b[1;32m    103\u001b[0m acc1, acc5 \u001b[39m=\u001b[39m accuracy(output, labels, topk\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m))\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/localdisk/maciej/code/neural-compressor/neural_compressor/model/torch_model.py:90\u001b[0m, in \u001b[0;36mPyTorchBaseModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pytorch model forward func.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/fx/graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/fx/graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.15:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> 5\u001b[0m     input_stem_0_conv \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_stem, \u001b[39m\"\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mconv(x);  x \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     input_stem_1_conv_conv_input_scale_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_stem_1_conv_conv_input_scale_0\n\u001b[1;32m      7\u001b[0m     input_stem_1_conv_conv_input_zero_point_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_stem_1_conv_conv_input_zero_point_0\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = NAS(config)\n",
    "results = agent.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Search Results in the Multi-Objective Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'macs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'macs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m cm \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mcm\u001b[39m.\u001b[39mget_cmap(\u001b[39m'\u001b[39m\u001b[39mviridis_r\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m count \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df_dynas))]\n\u001b[0;32m---> 14\u001b[0m ax\u001b[39m.\u001b[39mscatter(df_dynas[\u001b[39m'\u001b[39;49m\u001b[39mmacs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues, df_dynas[\u001b[39m'\u001b[39m\u001b[39mtop1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m'\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, c\u001b[39m=\u001b[39mcount, \n\u001b[1;32m     15\u001b[0m            cmap\u001b[39m=\u001b[39mcm, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDiscovered DNN Model\u001b[39m\u001b[39m'\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     16\u001b[0m ax\u001b[39m.\u001b[39mset_title(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIntel® Neural Compressor\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDynamic NAS (DyNAS)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSupernet:\u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mdynas\u001b[39m.\u001b[39msupernet\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m ax\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mMACs\u001b[39m\u001b[39m'\u001b[39m, fontsize\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m)\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/localdisk/maciej/miniconda3/envs/inc/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'macs'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "number_of_evals = 500\n",
    "df_dynas = pd.read_csv('results_resnet50_quant.csv')[:number_of_evals]\n",
    "df_dynas.columns = ['config', 'date', 'params', 'lat', 'model_size', 'top1']\n",
    "\n",
    "cm = plt.cm.get_cmap('viridis_r')\n",
    "count = [x for x in range(len(df_dynas))]\n",
    "\n",
    "ax.scatter(df_dynas['macs'].values, df_dynas['top1'].values, marker='^', alpha=0.8, c=count, \n",
    "           cmap=cm, label='Discovered DNN Model', s=10)\n",
    "ax.set_title(f'Intel® Neural Compressor\\nDynamic NAS (DyNAS)\\nSupernet:{config.dynas.supernet}')\n",
    "ax.set_xlabel('Latency (normalized)', fontsize=13)\n",
    "ax.set_ylabel('Top-1 Accuracy (%)', fontsize=13)\n",
    "ax.legend(fancybox=True, fontsize=10, framealpha=1, borderpad=0.2, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "#ax.set_ylim(72,77.5)\n",
    "\n",
    "# Eval Count bar\n",
    "norm = plt.Normalize(0, len(df_dynas))\n",
    "sm = ScalarMappable(norm=norm, cmap=cm)\n",
    "cbar = fig.colorbar(sm, ax=ax, shrink=0.85)\n",
    "cbar.ax.set_title(\"         Evaluation\\n  Count\", fontsize=8)\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Cai, H., Gan, C., & Han, S. (2020). Once for All: Train One Network and Specialize it for Efficient Deployment. ArXiv, abs/1908.09791.   \n",
    "[2] K. Deb, A. Pratap, S. Agarwal and T. Meyarivan, \"A fast and elitist multiobjective genetic algorithm: NSGA-II,\" in IEEE Transactions on Evolutionary Computation, vol. 6, no. 2, pp. 182-197, April 2002, doi: 10.1109/4235.996017. \n",
    "[3] Cummings, D., Sarah, A., Sridhar, S.N., Szankin, M., Muñoz, J.P., & Sundaresan, S. (2022). A Hardware-Aware Framework for Accelerating Neural Architecture Search Across Modalities. ArXiv, abs/2205.10358.   \n",
    "[4] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778.  \n",
    "[5] Howard, A.G., Sandler, M., Chu, G., Chen, L., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q.V., & Adam, H. (2019). Searching for MobileNetV3. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 1314-1324.    \n",
    "[6] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C. and Han, S., 2020. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187.    \n",
    "[7] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems, 30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynast_inc_pip_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "034f8a08a724a63543abaa4596714d81bf71b36e8b4dd0d5bf824a9fea1bc071"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
