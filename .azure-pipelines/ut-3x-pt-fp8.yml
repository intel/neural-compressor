trigger: none

pr:
  autoCancel: true
  drafts: false
  branches:
    include:
      - master
  paths:
    include:
      - .azure-pipelines/scripts/ut/3x/run_3x_pt_fp8.sh
      - .azure-pipelines/scripts/install_nc.sh
      - .azure-pipelines/ut-3x-pt-fp8.yml
      - .azure-pipelines/template/docker-template.yml
      - neural_compressor/common
      - neural_compressor/torch
      - neural_compressor/transformers
      - test/3x/torch/algorithms/fp8_quant
      - test/3x/torch/quantization/fp8_quant
      - test/3x/torch/quantization/weight_only/test_rtn.py
      - test/3x/torch/quantization/weight_only/test_load.py
      - setup.py
      - requirements_pt.txt

pool: GAUDI

variables:
  IMAGE_NAME: "neural-compressor"
  IMAGE_TAG: "py310"
  UPLOAD_PATH: $(Build.SourcesDirectory)/log_dir
  DOWNLOAD_PATH: $(Build.SourcesDirectory)/log_dir
  ARTIFACT_NAME: "UT_coverage_report_3x_pt_fp8"
  REPO: $(Build.Repository.Uri)

stages:
  - stage: Torch_habana
    displayName: Torch 3x Habana FP8
    dependsOn: []
    jobs:
      - job:
        displayName: Torch 3x Habana FP8
        steps:
          - template: template/ut-template.yml
            parameters:
              imageSource: "pull"
              dockerConfigName: "commonDockerConfig"
              utScriptFileName: "3x/run_3x_pt_fp8"
              uploadPath: $(UPLOAD_PATH)
              utArtifact: "ut_3x"

  - stage: Torch_habana_baseline
    displayName: Torch 3x Habana FP8 baseline
    dependsOn: []
    jobs:
      - job:
        displayName: Torch 3x Habana FP8 baseline
        steps:
          - template: template/ut-template.yml
            parameters:
              imageSource: "pull"
              dockerConfigName: "gitCloneDockerConfig"
              utScriptFileName: "3x/run_3x_pt_fp8"
              uploadPath: $(UPLOAD_PATH)
              utArtifact: "ut_3x_baseline"

  - stage: Coverage
    displayName: "Coverage Compare"
    pool:
      vmImage: "ubuntu-latest"
    dependsOn: [Torch_habana, Torch_habana_baseline]
    jobs:
      - job: CollectDatafiles
        steps:
          - script: |
              if [[ ! $(docker images | grep -i ${IMAGE_NAME}:${IMAGE_TAG}) ]]; then
                docker build -f ${BUILD_SOURCESDIRECTORY}/.azure-pipelines/docker/Dockerfile.devel -t ${IMAGE_NAME}:${IMAGE_TAG} .
              fi
              docker images | grep -i ${IMAGE_NAME}
              if [[ $? -ne 0 ]]; then
                echo "NO Such Repo"
                exit 1
              fi
            displayName: "Build develop docker image"

          - task: DownloadPipelineArtifact@2
            inputs:
              artifact:
              patterns: '*_coverage/.coverage'
              path: $(DOWNLOAD_PATH)

          - script: |
              echo "--- create container ---"
              docker run -d -it --name="collectLogs" -v ${BUILD_SOURCESDIRECTORY}:/neural-compressor  ${IMAGE_NAME}:${IMAGE_TAG} /bin/bash
              echo "--- docker ps ---"
              docker ps
              echo "--- collect logs ---"
              docker exec collectLogs /bin/bash  +x -c "cd /neural-compressor/.azure-pipelines/scripts \
              && bash install_nc.sh 3x_pt_fp8 \
              && bash ut/3x/collect_log_3x.sh 3x_pt_fp8"
            displayName: "Collect UT Coverage"

          - task: PublishCodeCoverageResults@2
            inputs:
              summaryFileLocation: $(Build.SourcesDirectory)/log_dir/coverage_PR/coverage.xml

          - task: PublishPipelineArtifact@1
            condition: succeededOrFailed()
            inputs:
              targetPath: $(UPLOAD_PATH)
              artifact: $(ARTIFACT_NAME)
              publishLocation: "pipeline"

          - task: Bash@3
            condition: always()
            inputs:
              targetType: "inline"
              script: |
                docker exec collectLogs bash -c "rm -fr /neural-compressor/* && rm -fr /neural-compressor/.* || true"
            displayName: "Docker clean up"
