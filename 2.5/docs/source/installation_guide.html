<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Installation &mdash; Intel® Neural Compressor 2.5 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="User Guide" href="user_guide.html" />
    <link rel="prev" title="Getting Started" href="get_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../versions.html">2.5▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-from-binary">Install from Binary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-from-source">Install from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-from-ai-kit">Install from AI Kit</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#system-requirements">System Requirements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#validated-hardware-environment">Validated Hardware Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">Intel® Neural Compressor supports CPUs based on Intel 64 architecture or compatible processors:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime">Intel® Neural Compressor quantized ONNX models support multiple hardware vendors through ONNX Runtime:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#validated-software-environment">Validated Software Environment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Installation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/source/installation_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h1>
<ol>
<li><p><a class="reference external" href="#installation">Installation</a></p>
<p>1.1. <a class="reference external" href="#prerequisites">Prerequisites</a></p>
<p>1.2. <a class="reference external" href="#install-from-binary">Install from Binary</a></p>
<p>1.3. <a class="reference external" href="#install-from-source">Install from Source</a></p>
<p>1.4. <a class="reference external" href="#install-from-ai-kit">Install from AI Kit</a></p>
</li>
<li><p><a class="reference external" href="#system-requirements">System Requirements</a></p>
<p>2.1. <a class="reference external" href="#validated-hardware-environment">Validated Hardware Environment</a></p>
<p>2.2. <a class="reference external" href="#validated-software-environment">Validated Software Environment</a></p>
</li>
</ol>
<section id="id1">
<h2>Installation<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h3>
<p>You can install Neural Compressor using one of three options: Install single component from binary or source, or get the Intel-optimized framework together with the library by installing the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html">Intel® oneAPI AI Analytics Toolkit</a>.</p>
<p>The following prerequisites and requirements must be satisfied for a successful installation:</p>
<ul class="simple">
<li><p>Python version: 3.8 or 3.9 or 3.10 or 3.11</p></li>
</ul>
<blockquote>
<div><p>Notes:</p>
<ul class="simple">
<li><p>If you get some build issues, please check <a class="reference external" href="faq.html">frequently asked questions</a> at first.</p></li>
</ul>
</div></blockquote>
</section>
<section id="install-from-binary">
<h3>Install from Binary<a class="headerlink" href="#install-from-binary" title="Link to this heading"></a></h3>
<ul>
<li><p>Install from Pypi</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install stable basic version from pypi</span>
pip<span class="w"> </span>install<span class="w"> </span>neural-compressor
</pre></div>
</div>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># [Experimental] install stable basic + PyTorch framework extension API from pypi </span>
pip<span class="w"> </span>install<span class="w"> </span>neural-compressor<span class="o">[</span>pt<span class="o">]</span>
</pre></div>
</div>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># [Experimental] install stable basic + TensorFlow framework extension API from pypi </span>
pip<span class="w"> </span>install<span class="w"> </span>neural-compressor<span class="o">[</span>tf<span class="o">]</span>
</pre></div>
</div>
</li>
<li><p>Install from test Pypi</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install nightly version</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/neural-compressor.git
<span class="nb">cd</span><span class="w"> </span>neural-compressor
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
<span class="c1"># install nightly basic version from pypi</span>
pip<span class="w"> </span>install<span class="w"> </span>-i<span class="w"> </span>https://test.pypi.org/simple/<span class="w"> </span>neural-compressor
</pre></div>
</div>
</li>
<li><p>Install from Conda</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install on Linux OS</span>
conda<span class="w"> </span>install<span class="w"> </span>opencv-python-headless<span class="w"> </span>-c<span class="w"> </span>fastai
conda<span class="w"> </span>install<span class="w"> </span>neural-compressor<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>intel
</pre></div>
</div>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install on Windows OS</span>
conda<span class="w"> </span>install<span class="w"> </span>pycocotools<span class="w"> </span>-c<span class="w"> </span>esri
conda<span class="w"> </span>install<span class="w"> </span>opencv-python-headless<span class="w"> </span>-c<span class="w"> </span>fastai
conda<span class="w"> </span>install<span class="w"> </span>neural-compressor<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-c<span class="w"> </span>intel
</pre></div>
</div>
</li>
</ul>
</section>
<section id="install-from-source">
<h3>Install from Source<a class="headerlink" href="#install-from-source" title="Link to this heading"></a></h3>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/neural-compressor.git
<span class="nb">cd</span><span class="w"> </span>neural-compressor
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
<span class="c1"># build with basic functionality</span>
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
</section>
<section id="install-from-ai-kit">
<h3>Install from AI Kit<a class="headerlink" href="#install-from-ai-kit" title="Link to this heading"></a></h3>
<p>The Intel® Neural Compressor library is released as part of the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html">Intel® oneAPI AI Analytics Toolkit</a> (AI Kit). The AI Kit provides a consolidated package of Intel’s latest deep learning and machine optimizations all in one place for ease of development. Along with Neural Compressor, the AI Kit includes Intel-optimized versions of deep learning frameworks (such as TensorFlow and PyTorch) and high-performing Python libraries to streamline end-to-end data science and AI workflows on Intel architectures.</p>
<p>The AI Kit is distributed through many common channels, including from Intel’s website, YUM, APT, Anaconda, and more. Select and <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/download.html">download</a> the AI Kit distribution package that’s best suited for you and follow the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Get Started Guide</a> for post-installation instructions.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Download</th>
<th>Guide</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/">Download AI Kit</a></td>
<td><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">AI Kit Get Started Guide</a></td>
</tr>
</tbody>
</table></section>
</section>
<section id="system-requirements">
<h2>System Requirements<a class="headerlink" href="#system-requirements" title="Link to this heading"></a></h2>
<section id="validated-hardware-environment">
<h3>Validated Hardware Environment<a class="headerlink" href="#validated-hardware-environment" title="Link to this heading"></a></h3>
<section id="intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">
<h4>Intel® Neural Compressor supports CPUs based on <a class="reference external" href="https://en.wikipedia.org/wiki/X86-64">Intel 64 architecture or compatible processors</a>:<a class="headerlink" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, Cooper Lake, Ice Lake, and Sapphire Rapids)</p></li>
<li><p>Intel Xeon CPU Max Series (formerly Sapphire Rapids HBM)</p></li>
</ul>
</section>
<section id="intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">
<h4>Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:<a class="headerlink" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Intel Data Center GPU Flex Series (formerly Arctic Sound-M)</p></li>
<li><p>Intel Data Center GPU Max Series (formerly Ponte Vecchio)</p></li>
</ul>
</section>
<section id="intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime">
<h4>Intel® Neural Compressor quantized ONNX models support multiple hardware vendors through ONNX Runtime:<a class="headerlink" href="#intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Intel CPU, AMD/ARM CPU, and NVidia GPU. Please refer to the validated model <a class="reference external" href="./validated_model_list.html#validated-onnx-qdq-int8-models-on-multiple-hardware-through-onnx-runtime">list</a>.</p></li>
</ul>
</section>
</section>
<section id="validated-software-environment">
<h3>Validated Software Environment<a class="headerlink" href="#validated-software-environment" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>OS version: CentOS 8.4, Ubuntu 22.04, MacOS Ventura 13.5, Windows 11</p></li>
<li><p>Python version: 3.8, 3.9, 3.10, 3.11</p></li>
</ul>
<table class="docutils">
<thead>
  <tr style="vertical-align: middle; text-align: center;">
    <th>Framework</th>
    <th>TensorFlow</th>
    <th>Intel<br>TensorFlow</th>
    <th>Intel®<br>Extension for<br>TensorFlow*</th>
    <th>PyTorch</th>
    <th>Intel®<br>Extension for<br>PyTorch*</th>
    <th>ONNX<br>Runtime</th>
  </tr>
</thead>
<tbody>
  <tr align="center">
    <th>Version</th>
    <td class="tg-7zrl"> <a href=https://github.com/tensorflow/tensorflow/tree/v2.15.0>2.15.0</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.14.1>2.14.1</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.13.1>2.13.1</a><br></td>
    <td class="tg-7zrl"> <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.14.0>2.14.0</a><br>
    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.13.0>2.13.0</a><br></td>
    <td class="tg-7zrl"> <a href=https://github.com/intel/intel-extension-for-tensorflow/tree/v2.14.0.1>2.14.0.1</a><br>
    <a href=https://github.com/intel/intel-extension-for-tensorflow/tree/v2.13.0.0>2.13.0.0</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/pytorch/pytorch/tree/v2.2.1>2.2.1</a><br>
    <a href=https://github.com/pytorch/pytorch/tree/v2.1.0>2.1.0</a><br>
    <a href=https://github.com/pytorch/pytorch/tree/v2.0.1>2.0.1</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/intel/intel-extension-for-pytorch/tree/v2.2.0%2Bcpu>2.2.0</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.100%2Bcpu>2.1.100</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100%2Bcpu>2.0.100</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/microsoft/onnxruntime/tree/v1.17.1>1.17.1</a><br>
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.16.3>1.16.3</a><br>    
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.15.1>1.15.1</a><br></td>
  </tr>
</tbody>
</table><blockquote>
<div><p><strong>Note:</strong>
Set the environment variable <code class="docutils literal notranslate"><span class="pre">TF_ENABLE_ONEDNN_OPTS=1</span></code> to enable oneDNN optimizations if you are using TensorFlow before v2.9. oneDNN is the default for TensorFlow since <a class="reference external" href="https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0">v2.9</a> (<a class="reference external" href="https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html">Intel Cascade Lake</a> and newer CPUs).</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="get_started.html" class="btn btn-neutral float-left" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="user_guide.html" class="btn btn-neutral float-right" title="User Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f81ca92b640> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>